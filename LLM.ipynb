{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project LLM-based NER & TS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 命名实体识别任务\n",
    "\n",
    "## 1.1 数据处理\n",
    "\n",
    "本实验采用的 CHisIEC 古文命名实体识别数据集包含古文文本及其对应的命名实体标签。数据以 CoNLL 格式存储，每行包含一个词及其标签，句子之间用空行分隔。实体标签采用 BIOES 标注方式。为了适配基于prompt的 LLM 方法，需要将 CoNLL 格式的数据转换为 JSONL 格式，且每个样本包含针对不同实体类型的指令和期望输出。\n",
    "\n",
    "主要步骤如下：\n",
    "\n",
    "1. **读取 CoNLL 数据**：逐行读取数据文件，并按句子分割。\n",
    "2. **实体提取**：根据标签（BIOES）提取实体，并按类型分类。\n",
    "3. **构造提示样本**：针对每种实体类型，生成相应的指令和输出。若某类型实体不存在，则输出 `null`。\n",
    "\n",
    "通过上述步骤，训练集和验证集被转换为适用于 LLM 微调的 JSONL 格式文件，便于后续的模型训练。\n",
    "\n",
    "## 1.2 模型微调\n",
    "\n",
    "初步实验发现，若直接采用与第二个任务文本摘要相同的方式对 Qwen 原始模型直接输入 prompt 并生成输出进行命名实体识别，其效果非常差，`F1-macro` 通常不足 `0.1`。因此，尝试对 Qwen 模型进行微调以提高分数。\n",
    "\n",
    "微调过程通过 `SftArguments` 配置，主要参数包括：\n",
    "\n",
    "- `model_type`：选择的基础模型类型。\n",
    "- `dataset`：训练集路径。\n",
    "- `val_dataset`：验证集路径。\n",
    "- `per_device_train_batch_size`：训练批次大小，设置为 16。\n",
    "- `num_train_epochs`：训练轮次，设置为 7 轮。\n",
    "- `output_dir`：模型输出目录，设置为 `output`。\n",
    "\n",
    "调用 `SftArguments` 配置并执行 `sft_main(sft_args)` 函数，开始模型微调。微调完成后，记录最佳模型的检查点路径 `best_model_checkpoint`，用于后续的推理阶段。\n",
    "\n",
    "微调过程中，监控多个训练轮次中验证集的性能，最终选择在验证集上表现最优的模型作为最佳模型，存储在 `output` 目录下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Successfully registered `C:\\Users\\IScream\\anaconda3\\envs\\virtual\\Lib\\site-packages\\swift\\llm\\data\\dataset_info.json`\n",
      "[INFO:swift] No vLLM installed, if you are using vLLM, you will get `ImportError: cannot import name 'get_vllm_engine' from 'swift.llm'`\n",
      "[INFO:swift] No LMDeploy installed, if you are using LMDeploy, you will get `ImportError: cannot import name 'prepare_lmdeploy_engine_template' from 'swift.llm'`\n",
      "[INFO:swift] Using val_dataset, ignoring dataset_test_ratio\n",
      "[INFO:swift] Setting template_type: qwen2_5\n",
      "[INFO:swift] Setting args.lazy_tokenize: False\n",
      "[INFO:swift] Setting args.dataloader_num_workers: 0\n",
      "[INFO:swift] output_dir: C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828\n",
      "[INFO:swift] Start time of running main: 2024-11-17 13:28:28.924907\n",
      "[INFO:swift] args: SftArguments(model_type='qwen2_5-0_5b-instruct', model_id_or_path='qwen/Qwen2.5-0.5B-Instruct', model_revision='master', full_determinism=False, sft_type='lora', freeze_parameters=[], freeze_vit=False, freeze_parameters_ratio=0.0, additional_trainable_parameters=[], tuner_backend='peft', template_type='qwen2_5', output_dir='C:\\\\Users\\\\IScream\\\\Desktop\\\\study\\\\自然语言处理\\\\PJ-基于LLM的命名实体识别和摘要任务\\\\data\\\\output\\\\qwen2_5-0_5b-instruct\\\\v20-20241117-132828', add_output_dir_suffix=True, ddp_backend=None, ddp_find_unused_parameters=None, ddp_broadcast_buffers=None, ddp_timeout=1800, seed=42, resume_from_checkpoint=None, resume_only_model=False, ignore_data_skip=False, dtype='bf16', packing=False, train_backend='transformers', tp=1, pp=1, min_lr=None, sequence_parallel=False, model_kwargs={}, loss_name=None, dataset=['./train_separate_prompts.jsonl'], val_dataset=['./dev_separate_prompts.jsonl'], dataset_seed=42, dataset_test_ratio=0.0, use_loss_scale=False, loss_scale_config_path='C:\\\\Users\\\\IScream\\\\anaconda3\\\\envs\\\\virtual\\\\Lib\\\\site-packages\\\\swift\\\\llm\\\\agent\\\\default_loss_scale_config.json', system=None, tools_prompt='react_en', max_length=2048, truncation_strategy='delete', check_dataset_strategy='none', streaming=False, streaming_val_size=0, streaming_buffer_size=16384, model_name=[None, None], model_author=[None, None], quant_method=None, quantization_bit=0, hqq_axis=0, hqq_dynamic_config_path=None, bnb_4bit_comp_dtype='bf16', bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, rescale_image=-1, target_modules=['q_proj', 'k_proj', 'v_proj'], target_regex=None, modules_to_save=[], lora_rank=8, lora_alpha=32, lora_dropout=0.05, lora_bias_trainable='none', lora_dtype='AUTO', lora_lr_ratio=None, use_rslora=False, use_dora=False, init_lora_weights='true', fourier_n_frequency=2000, fourier_scaling=300.0, rope_scaling=None, boft_block_size=4, boft_block_num=0, boft_n_butterfly_factor=1, boft_dropout=0.0, vera_rank=256, vera_projection_prng_key=0, vera_dropout=0.0, vera_d_initial=0.1, adapter_act='gelu', adapter_length=128, use_galore=False, galore_target_modules=None, galore_rank=128, galore_update_proj_gap=50, galore_scale=1.0, galore_proj_type='std', galore_optim_per_parameter=False, galore_with_embedding=False, galore_quantization=False, galore_proj_quant=False, galore_proj_bits=4, galore_proj_group_size=256, galore_cos_threshold=0.4, galore_gamma_proj=2, galore_queue_size=5, adalora_target_r=8, adalora_init_r=12, adalora_tinit=0, adalora_tfinal=0, adalora_deltaT=1, adalora_beta1=0.85, adalora_beta2=0.85, adalora_orth_reg_weight=0.5, ia3_feedforward_modules=[], llamapro_num_new_blocks=4, llamapro_num_groups=None, neftune_noise_alpha=None, neftune_backend='transformers', lisa_activated_layers=0, lisa_step_interval=20, reft_layer_key=None, reft_layers=None, reft_rank=4, reft_intervention_type='LoreftIntervention', reft_args=None, use_liger=False, gradient_checkpointing=True, vit_use_gc=True, deepspeed=None, batch_size=16, eval_batch_size=16, auto_find_batch_size=False, num_train_epochs=7, max_steps=-1, optim='adamw_torch', adam_beta1=0.9, adam_beta2=0.95, adam_epsilon=1e-08, learning_rate=0.0001, weight_decay=0.1, gradient_accumulation_steps=1, max_grad_norm=1, predict_with_generate=False, lr_scheduler_type='cosine', lr_scheduler_kwargs={}, warmup_ratio=0.05, warmup_steps=0, eval_steps=50, save_steps=50, save_only_model=False, save_total_limit=2, logging_steps=5, acc_steps=1, dataloader_num_workers=0, dataloader_pin_memory=True, dataloader_drop_last=False, push_to_hub=False, hub_model_id=None, hub_token=None, hub_private_repo=False, hub_strategy='every_save', test_oom_error=False, disable_tqdm=False, lazy_tokenize=False, preprocess_num_proc=1, use_flash_attn=None, ignore_args_error=False, check_model_is_latest=True, logging_dir='C:\\\\Users\\\\IScream\\\\Desktop\\\\study\\\\自然语言处理\\\\PJ-基于LLM的命名实体识别和摘要任务\\\\data\\\\output\\\\qwen2_5-0_5b-instruct\\\\v20-20241117-132828/runs', report_to=['tensorboard'], acc_strategy='token', save_on_each_node=False, evaluation_strategy='steps', save_strategy='steps', save_safetensors=True, gpu_memory_fraction=None, include_num_input_tokens_seen=False, local_repo_path=None, custom_register_path=None, custom_dataset_info=None, device_map_config=None, device_max_memory=[], max_new_tokens=2048, do_sample=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, fsdp='', fsdp_config=None, sequence_parallel_size=1, model_layer_cls_name=None, metric_warmup_step=0, fsdp_num=1, per_device_train_batch_size=16, per_device_eval_batch_size=None, eval_strategy=None, self_cognition_sample=0, train_dataset_mix_ratio=0.0, train_dataset_mix_ds=['ms-bench'], train_dataset_sample=-1, val_dataset_sample=None, safe_serialization=None, only_save_model=None, neftune_alpha=None, deepspeed_config_path=None, model_cache_dir=None, lora_dropout_p=None, lora_target_modules=[], lora_target_regex=None, lora_modules_to_save=[], boft_target_modules=[], boft_modules_to_save=[], vera_target_modules=[], vera_modules_to_save=[], ia3_target_modules=[], ia3_modules_to_save=[], custom_train_dataset_path=[], custom_val_dataset_path=[], device_map_config_path=None, push_hub_strategy=None)\n",
      "[INFO:swift] Global seed set to 42\n",
      "[INFO:swift] Downloading the model from ModelScope Hub, model_id: qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_count: 1\n",
      "rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING:modelscope] Using branch: master as version is unstable, use with caution\n",
      "[INFO:swift] Loading the model using model_dir: C:\\Users\\IScream\\.cache\\modelscope\\hub\\qwen\\Qwen2___5-0___5B-Instruct\n",
      "[INFO:swift] model_kwargs: {'device_map': 'cuda:0'}\n",
      "[INFO:swift] model.max_model_len: 32768\n",
      "[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}\n",
      "[INFO:swift] model_config: Qwen2Config {\n",
      "  \"_name_or_path\": \"C:\\\\Users\\\\IScream\\\\.cache\\\\modelscope\\\\hub\\\\qwen\\\\Qwen2___5-0___5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO:swift] model.generation_config: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO:swift] Setting model.config.use_cache: False\n",
      "[INFO:swift] target_modules: ['q_proj', 'k_proj', 'v_proj']\n",
      "[INFO:swift] modules_to_save: []\n",
      "[INFO:swift] lora_config: get_wrapped_class.<locals>.PeftWrapper(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='C:\\\\Users\\\\IScream\\\\.cache\\\\modelscope\\\\hub\\\\qwen\\\\Qwen2___5-0___5B-Instruct', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'q_proj', 'k_proj', 'v_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=[], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)\n",
      "[INFO:swift] [base_model.model.model.embed_tokens.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.self_attn.o_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.gate_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.up_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.mlp.down_proj.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.input_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.0.post_attention_layernorm.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] ...\n",
      "[INFO:swift] PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151936, 896)\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2SdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=896, out_features=896, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=896, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "              (rotary_emb): Qwen2RotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "              (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "              (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (rotary_emb): Qwen2RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[INFO:swift] PeftModelForCausalLM: 494.7700M Params (0.7373M Trainable [0.1490%]), 0.0008M Buffers.\n",
      "[INFO:swift] system: You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "[INFO:swift] args.lazy_tokenize: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d8d527516843a2b1c02ad366dcbb1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68e9dccdb534ec49a242ca7eeab06a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506a66e707a944c18e8d0a80b39c991b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752fd8e02a6d4aa0a4925cf66a6ecb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfebe8d7cb24f6fbe98af2a51976f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab8b9d12f5d466cba51d855be4a8bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] train_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 872\n",
      "})\n",
      "[INFO:swift] val_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 872\n",
      "})\n",
      "[INFO:swift] [INPUT_IDS] [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 14880, 45181, 87752, 109949, 15946, 102450, 62926, 107439, 20221, 55338, 100623, 13072, 3837, 37029, 6, 3837, 6, 109619, 3837, 62244, 69184, 46448, 31526, 6, 2921, 6, 8997, 57191, 23031, 56007, 56137, 99462, 3837, 75598, 227, 104769, 5122, 12881, 99818, 99755, 100541, 99867, 100426, 3837, 17340, 35568, 53930, 40981, 107570, 3837, 100216, 107656, 102394, 3837, 108558, 49828, 57218, 17340, 35568, 99399, 40981, 107570, 113722, 75764, 41146, 100966, 27641, 101073, 1773, 33447, 44636, 100401, 52183, 68536, 99194, 15362, 236, 57566, 53930, 1773, 99966, 101269, 106721, 121720, 3837, 119163, 99816, 44991, 8903, 3837, 32555, 99271, 99412, 23031, 32571, 100839, 101326, 101268, 53930, 3837, 101568, 29767, 99468, 99966, 40916, 44991, 64643, 5373, 62926, 54039, 26288, 71268, 99625, 3837, 121073, 104769, 101739, 1773, 151645, 198, 151644, 77091, 198, 56137, 99462, 3837, 44636, 100401, 3837, 101739, 151645]\n",
      "[INFO:swift] [INPUT] <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "请从以下句子中识别并提取出所有的人名，使用'，'分割，如果不存在则返回'null'。\n",
      "或以问至德，荅曰：「夫庆赏刑罪，人主之权柄，凡为人臣，岂得与人主争权柄哉！」其慎密如此。后高宗知而深歎美之。仪凤四年薨，辍朝三日，使百官以次赴宅哭之，赠开府仪同三司、并州大都督，谥曰恭。<|im_end|>\n",
      "<|im_start|>assistant\n",
      "至德，高宗，恭<|im_end|>\n",
      "[INFO:swift] [LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 56137, 99462, 3837, 44636, 100401, 3837, 101739, 151645]\n",
      "[INFO:swift] [LABELS] [-100 * 147]至德，高宗，恭<|im_end|>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b577a80fc024a31915a29b5b5c3f85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47b9d73b85d4f02ac80b8fc575de5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Dataset Token Length: 120.368119±26.191007, min=62.000000, max=190.000000, size=872\n",
      "[INFO:swift] Dataset Token Length: 120.368119±26.191007, min=62.000000, max=190.000000, size=872\n",
      "[INFO:swift] training_args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "acc_strategy=token,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': False, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.95,\n",
      "adam_epsilon=1e-08,\n",
      "additional_saved_files=[],\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=42,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=50,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      ",\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828/runs,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "loss_name=None,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.COSINE,\n",
      "max_grad_norm=1,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "metric_warmup_step=0,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=7,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=50,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_dataset_sample=-1,\n",
      "train_sampler_random=True,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      ")\n",
      "[INFO:swift] The SftArguments will be saved in: C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828\\sft_args.json\n",
      "[INFO:swift] The Seq2SeqTrainingArguments will be saved in: C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828\\training_args.json\n",
      "[INFO:swift] The logging file will be saved in: C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828\\logging.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a19e1d9ff8f4b06a0d03b819242d857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|                                                                                   | 0/385 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.62735462, 'acc': 0.70689654, 'grad_norm': 18.16116524, 'learning_rate': 5e-06, 'memory(GiB)': 6.34, 'train_speed(iter/s)': 0.586995, 'epoch': 0.02, 'global_step/max_steps': '1/385', 'percentage': '0.26%', 'elapsed_time': '1s', 'remaining_time': '9m 35s'}\n",
      "{'loss': 2.26796722, 'acc': 0.60275841, 'grad_norm': 46.80228043, 'learning_rate': 2.5e-05, 'memory(GiB)': 10.97, 'train_speed(iter/s)': 0.484662, 'epoch': 0.09, 'global_step/max_steps': '5/385', 'percentage': '1.30%', 'elapsed_time': '10s', 'remaining_time': '12m 48s'}\n",
      "{'loss': 1.99840488, 'acc': 0.65477514, 'grad_norm': 21.38861465, 'learning_rate': 5e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.394219, 'epoch': 0.18, 'global_step/max_steps': '10/385', 'percentage': '2.60%', 'elapsed_time': '25s', 'remaining_time': '15m 43s'}\n",
      "{'loss': 1.87202034, 'acc': 0.61484923, 'grad_norm': 11.37532425, 'learning_rate': 7.5e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.301021, 'epoch': 0.27, 'global_step/max_steps': '15/385', 'percentage': '3.90%', 'elapsed_time': '49s', 'remaining_time': '20m 24s'}\n",
      "{'loss': 1.37240763, 'acc': 0.66565008, 'grad_norm': 17.85288239, 'learning_rate': 0.0001, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.354081, 'epoch': 0.36, 'global_step/max_steps': '20/385', 'percentage': '5.19%', 'elapsed_time': '56s', 'remaining_time': '17m 7s'}\n",
      "{'loss': 1.18439636, 'acc': 0.72574081, 'grad_norm': 11.1031847, 'learning_rate': 9.995e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.335054, 'epoch': 0.45, 'global_step/max_steps': '25/385', 'percentage': '6.49%', 'elapsed_time': '1m 14s', 'remaining_time': '17m 51s'}\n",
      "{'loss': 1.18723307, 'acc': 0.71219044, 'grad_norm': 10.42777729, 'learning_rate': 9.981e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.31628, 'epoch': 0.55, 'global_step/max_steps': '30/385', 'percentage': '7.79%', 'elapsed_time': '1m 34s', 'remaining_time': '18m 40s'}\n",
      "{'loss': 1.1493432, 'acc': 0.71547227, 'grad_norm': 14.50942707, 'learning_rate': 9.958e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.319949, 'epoch': 0.64, 'global_step/max_steps': '35/385', 'percentage': '9.09%', 'elapsed_time': '1m 49s', 'remaining_time': '18m 11s'}\n",
      "{'loss': 1.03534031, 'acc': 0.71852708, 'grad_norm': 9.26693249, 'learning_rate': 9.926e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.334828, 'epoch': 0.73, 'global_step/max_steps': '40/385', 'percentage': '10.39%', 'elapsed_time': '1m 59s', 'remaining_time': '17m 8s'}\n",
      "{'loss': 0.99977961, 'acc': 0.72133474, 'grad_norm': 10.63405704, 'learning_rate': 9.885e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.33133, 'epoch': 0.82, 'global_step/max_steps': '45/385', 'percentage': '11.69%', 'elapsed_time': '2m 15s', 'remaining_time': '17m 4s'}\n",
      "{'loss': 0.95044708, 'acc': 0.74020338, 'grad_norm': 10.68346596, 'learning_rate': 9.834e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.328399, 'epoch': 0.91, 'global_step/max_steps': '50/385', 'percentage': '12.99%', 'elapsed_time': '2m 32s', 'remaining_time': '16m 58s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c3a3ed2bbe4bc294184bac83628e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|                                                                                      | 0/55 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828\\checkpoint-50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.83086431, 'eval_acc': 0.76423211, 'eval_runtime': 50.8639, 'eval_samples_per_second': 17.144, 'eval_steps_per_second': 1.081, 'epoch': 0.91, 'global_step/max_steps': '50/385', 'percentage': '12.99%', 'elapsed_time': '3m 22s', 'remaining_time': '22m 39s'}\n",
      "{'loss': 0.9904398, 'acc': 0.74184752, 'grad_norm': 12.63731956, 'learning_rate': 9.775e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.254405, 'epoch': 1.0, 'global_step/max_steps': '55/385', 'percentage': '14.29%', 'elapsed_time': '3m 35s', 'remaining_time': '21m 35s'}\n",
      "{'loss': 0.71842484, 'acc': 0.77823877, 'grad_norm': 5.7387166, 'learning_rate': 9.707e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.253615, 'epoch': 1.09, 'global_step/max_steps': '60/385', 'percentage': '15.58%', 'elapsed_time': '3m 56s', 'remaining_time': '21m 20s'}\n",
      "{'loss': 0.86882486, 'acc': 0.75797563, 'grad_norm': 10.92772865, 'learning_rate': 9.63e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.25346, 'epoch': 1.18, 'global_step/max_steps': '65/385', 'percentage': '16.88%', 'elapsed_time': '4m 16s', 'remaining_time': '21m 1s'}\n",
      "{'loss': 0.80892611, 'acc': 0.78827868, 'grad_norm': 7.78542233, 'learning_rate': 9.544e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.252187, 'epoch': 1.27, 'global_step/max_steps': '70/385', 'percentage': '18.18%', 'elapsed_time': '4m 37s', 'remaining_time': '20m 48s'}\n",
      "{'loss': 0.64452767, 'acc': 0.8139926, 'grad_norm': 12.26165199, 'learning_rate': 9.45e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.254987, 'epoch': 1.36, 'global_step/max_steps': '75/385', 'percentage': '19.48%', 'elapsed_time': '4m 53s', 'remaining_time': '20m 14s'}\n",
      "{'loss': 0.7625205, 'acc': 0.78965483, 'grad_norm': 9.85212517, 'learning_rate': 9.348e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.261988, 'epoch': 1.45, 'global_step/max_steps': '80/385', 'percentage': '20.78%', 'elapsed_time': '5m 5s', 'remaining_time': '19m 23s'}\n",
      "{'loss': 0.70277557, 'acc': 0.78989925, 'grad_norm': 6.9032135, 'learning_rate': 9.238e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.262966, 'epoch': 1.55, 'global_step/max_steps': '85/385', 'percentage': '22.08%', 'elapsed_time': '5m 23s', 'remaining_time': '19m 0s'}\n",
      "{'loss': 0.74327908, 'acc': 0.79247432, 'grad_norm': 14.26238728, 'learning_rate': 9.12e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.267029, 'epoch': 1.64, 'global_step/max_steps': '90/385', 'percentage': '23.38%', 'elapsed_time': '5m 36s', 'remaining_time': '18m 24s'}\n",
      "{'loss': 0.74261565, 'acc': 0.77261949, 'grad_norm': 13.76395607, 'learning_rate': 8.994e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.27034, 'epoch': 1.73, 'global_step/max_steps': '95/385', 'percentage': '24.68%', 'elapsed_time': '5m 51s', 'remaining_time': '17m 52s'}\n",
      "{'loss': 0.70924244, 'acc': 0.82121334, 'grad_norm': 8.13764858, 'learning_rate': 8.861e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.276877, 'epoch': 1.82, 'global_step/max_steps': '100/385', 'percentage': '25.97%', 'elapsed_time': '6m 0s', 'remaining_time': '17m 8s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfb6ec357f4483e92704d9f0fc9e493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|                                                                                      | 0/55 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828\\checkpoint-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5973081, 'eval_acc': 0.82445543, 'eval_runtime': 50.5057, 'eval_samples_per_second': 17.265, 'eval_steps_per_second': 1.089, 'epoch': 1.82, 'global_step/max_steps': '100/385', 'percentage': '25.97%', 'elapsed_time': '6m 51s', 'remaining_time': '19m 32s'}\n",
      "{'loss': 0.64538589, 'acc': 0.80908537, 'grad_norm': 14.40019321, 'learning_rate': 8.721e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.247123, 'epoch': 1.91, 'global_step/max_steps': '105/385', 'percentage': '27.27%', 'elapsed_time': '7m 4s', 'remaining_time': '18m 52s'}\n",
      "{'loss': 0.6702137, 'acc': 0.8039814, 'grad_norm': 14.620924, 'learning_rate': 8.573e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.251798, 'epoch': 2.0, 'global_step/max_steps': '110/385', 'percentage': '28.57%', 'elapsed_time': '7m 16s', 'remaining_time': '18m 11s'}\n",
      "{'loss': 0.52619824, 'acc': 0.84932652, 'grad_norm': 7.48117304, 'learning_rate': 8.42e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.253504, 'epoch': 2.09, 'global_step/max_steps': '115/385', 'percentage': '29.87%', 'elapsed_time': '7m 33s', 'remaining_time': '17m 44s'}\n",
      "{'loss': 0.5484971, 'acc': 0.85254698, 'grad_norm': 9.73379135, 'learning_rate': 8.259e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.258361, 'epoch': 2.18, 'global_step/max_steps': '120/385', 'percentage': '31.17%', 'elapsed_time': '7m 44s', 'remaining_time': '17m 5s'}\n",
      "{'loss': 0.45434194, 'acc': 0.8373045, 'grad_norm': 10.48783112, 'learning_rate': 8.093e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.26185, 'epoch': 2.27, 'global_step/max_steps': '125/385', 'percentage': '32.47%', 'elapsed_time': '7m 57s', 'remaining_time': '16m 32s'}\n",
      "{'loss': 0.63194499, 'acc': 0.80359917, 'grad_norm': 15.26862812, 'learning_rate': 7.921e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.262519, 'epoch': 2.36, 'global_step/max_steps': '130/385', 'percentage': '33.77%', 'elapsed_time': '8m 14s', 'remaining_time': '16m 10s'}\n",
      "{'loss': 0.61631112, 'acc': 0.81917591, 'grad_norm': 11.45943737, 'learning_rate': 7.744e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.263272, 'epoch': 2.45, 'global_step/max_steps': '135/385', 'percentage': '35.06%', 'elapsed_time': '8m 32s', 'remaining_time': '15m 49s'}\n",
      "{'loss': 0.5789319, 'acc': 0.83194208, 'grad_norm': 9.94352913, 'learning_rate': 7.562e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.26392, 'epoch': 2.55, 'global_step/max_steps': '140/385', 'percentage': '36.36%', 'elapsed_time': '8m 50s', 'remaining_time': '15m 27s'}\n",
      "{'loss': 0.62621727, 'acc': 0.81204672, 'grad_norm': 9.18952942, 'learning_rate': 7.375e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.265921, 'epoch': 2.64, 'global_step/max_steps': '145/385', 'percentage': '37.66%', 'elapsed_time': '9m 5s', 'remaining_time': '15m 2s'}\n",
      "{'loss': 0.46092381, 'acc': 0.84430666, 'grad_norm': 9.17795753, 'learning_rate': 7.183e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.270167, 'epoch': 2.73, 'global_step/max_steps': '150/385', 'percentage': '38.96%', 'elapsed_time': '9m 15s', 'remaining_time': '14m 29s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c00c88437164579af38c6a77a1edcd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|                                                                                      | 0/55 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828\\checkpoint-150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46459627, 'eval_acc': 0.86106535, 'eval_runtime': 50.3844, 'eval_samples_per_second': 17.307, 'eval_steps_per_second': 1.092, 'epoch': 2.73, 'global_step/max_steps': '150/385', 'percentage': '38.96%', 'elapsed_time': '10m 5s', 'remaining_time': '15m 48s'}\n",
      "{'loss': 0.54609728, 'acc': 0.83016586, 'grad_norm': 9.54401779, 'learning_rate': 6.988e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.249165, 'epoch': 2.82, 'global_step/max_steps': '155/385', 'percentage': '40.26%', 'elapsed_time': '10m 21s', 'remaining_time': '15m 22s'}\n",
      "{'loss': 0.55752196, 'acc': 0.84298687, 'grad_norm': 8.8589201, 'learning_rate': 6.788e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.25137, 'epoch': 2.91, 'global_step/max_steps': '160/385', 'percentage': '41.56%', 'elapsed_time': '10m 36s', 'remaining_time': '14m 54s'}\n",
      "{'loss': 0.5170064, 'acc': 0.84722223, 'grad_norm': 8.2065115, 'learning_rate': 6.586e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.254046, 'epoch': 3.0, 'global_step/max_steps': '165/385', 'percentage': '42.86%', 'elapsed_time': '10m 49s', 'remaining_time': '14m 25s'}\n",
      "{'loss': 0.38231149, 'acc': 0.88740387, 'grad_norm': 11.74018574, 'learning_rate': 6.38e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.255072, 'epoch': 3.09, 'global_step/max_steps': '170/385', 'percentage': '44.16%', 'elapsed_time': '11m 6s', 'remaining_time': '14m 2s'}\n",
      "{'loss': 0.43760438, 'acc': 0.89435768, 'grad_norm': 6.76675892, 'learning_rate': 6.172e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.257033, 'epoch': 3.18, 'global_step/max_steps': '175/385', 'percentage': '45.45%', 'elapsed_time': '11m 20s', 'remaining_time': '13m 36s'}\n",
      "{'loss': 0.36840029, 'acc': 0.88441944, 'grad_norm': 6.28509283, 'learning_rate': 5.962e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.258511, 'epoch': 3.27, 'global_step/max_steps': '180/385', 'percentage': '46.75%', 'elapsed_time': '11m 36s', 'remaining_time': '13m 12s'}\n",
      "{'loss': 0.45694084, 'acc': 0.84613628, 'grad_norm': 7.09648991, 'learning_rate': 5.75e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.259577, 'epoch': 3.36, 'global_step/max_steps': '185/385', 'percentage': '48.05%', 'elapsed_time': '11m 52s', 'remaining_time': '12m 50s'}\n",
      "{'loss': 0.48924847, 'acc': 0.83833981, 'grad_norm': 12.03480721, 'learning_rate': 5.537e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.262952, 'epoch': 3.45, 'global_step/max_steps': '190/385', 'percentage': '49.35%', 'elapsed_time': '12m 2s', 'remaining_time': '12m 21s'}\n",
      "{'loss': 0.50001101, 'acc': 0.87508011, 'grad_norm': 8.92395878, 'learning_rate': 5.323e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.265174, 'epoch': 3.55, 'global_step/max_steps': '195/385', 'percentage': '50.65%', 'elapsed_time': '12m 15s', 'remaining_time': '11m 56s'}\n",
      "{'loss': 0.38508427, 'acc': 0.89230566, 'grad_norm': 6.85355186, 'learning_rate': 5.108e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.265413, 'epoch': 3.64, 'global_step/max_steps': '200/385', 'percentage': '51.95%', 'elapsed_time': '12m 33s', 'remaining_time': '11m 36s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2dcb6250bb4f6d853887e02fc125a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|                                                                                      | 0/55 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828\\checkpoint-200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.38500726, 'eval_acc': 0.88541095, 'eval_runtime': 51.0478, 'eval_samples_per_second': 17.082, 'eval_steps_per_second': 1.077, 'epoch': 3.64, 'global_step/max_steps': '200/385', 'percentage': '51.95%', 'elapsed_time': '13m 24s', 'remaining_time': '12m 24s'}\n",
      "{'loss': 0.55170093, 'acc': 0.86086512, 'grad_norm': 8.07089424, 'learning_rate': 4.892e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.2503, 'epoch': 3.73, 'global_step/max_steps': '205/385', 'percentage': '53.25%', 'elapsed_time': '13m 38s', 'remaining_time': '11m 58s'}\n",
      "{'loss': 0.39737477, 'acc': 0.88505917, 'grad_norm': 9.45807552, 'learning_rate': 4.677e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.253288, 'epoch': 3.82, 'global_step/max_steps': '210/385', 'percentage': '54.55%', 'elapsed_time': '13m 48s', 'remaining_time': '11m 30s'}\n",
      "{'loss': 0.45714045, 'acc': 0.87477036, 'grad_norm': 8.63550854, 'learning_rate': 4.463e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.25225, 'epoch': 3.91, 'global_step/max_steps': '215/385', 'percentage': '55.84%', 'elapsed_time': '14m 12s', 'remaining_time': '11m 13s'}\n",
      "{'loss': 0.45660129, 'acc': 0.84783401, 'grad_norm': 11.4673624, 'learning_rate': 4.25e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.252958, 'epoch': 4.0, 'global_step/max_steps': '220/385', 'percentage': '57.14%', 'elapsed_time': '14m 29s', 'remaining_time': '10m 52s'}\n",
      "{'loss': 0.43139448, 'acc': 0.86930981, 'grad_norm': 12.36765957, 'learning_rate': 4.038e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.253794, 'epoch': 4.09, 'global_step/max_steps': '225/385', 'percentage': '58.44%', 'elapsed_time': '14m 46s', 'remaining_time': '10m 30s'}\n",
      "{'loss': 0.36528459, 'acc': 0.88278627, 'grad_norm': 8.80066395, 'learning_rate': 3.828e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.254838, 'epoch': 4.18, 'global_step/max_steps': '230/385', 'percentage': '59.74%', 'elapsed_time': '15m 2s', 'remaining_time': '10m 8s'}\n",
      "{'loss': 0.3270066, 'acc': 0.90980377, 'grad_norm': 7.11953926, 'learning_rate': 3.62e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.255219, 'epoch': 4.27, 'global_step/max_steps': '235/385', 'percentage': '61.04%', 'elapsed_time': '15m 20s', 'remaining_time': '9m 47s'}\n",
      "{'loss': 0.26259944, 'acc': 0.90872593, 'grad_norm': 6.41151524, 'learning_rate': 3.414e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.256885, 'epoch': 4.36, 'global_step/max_steps': '240/385', 'percentage': '62.34%', 'elapsed_time': '15m 34s', 'remaining_time': '9m 24s'}\n",
      "{'loss': 0.41837955, 'acc': 0.8737462, 'grad_norm': 8.42916489, 'learning_rate': 3.212e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.25923, 'epoch': 4.45, 'global_step/max_steps': '245/385', 'percentage': '63.64%', 'elapsed_time': '15m 44s', 'remaining_time': '8m 59s'}\n",
      "{'loss': 0.31391835, 'acc': 0.90984793, 'grad_norm': 8.51598167, 'learning_rate': 3.012e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.258546, 'epoch': 4.55, 'global_step/max_steps': '250/385', 'percentage': '64.94%', 'elapsed_time': '16m 6s', 'remaining_time': '8m 42s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ba8f367dfe4b588046d0c4f0b01e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|                                                                                      | 0/55 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828\\checkpoint-250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33722097, 'eval_acc': 0.89511258, 'eval_runtime': 50.874, 'eval_samples_per_second': 17.14, 'eval_steps_per_second': 1.081, 'epoch': 4.55, 'global_step/max_steps': '250/385', 'percentage': '64.94%', 'elapsed_time': '16m 57s', 'remaining_time': '9m 9s'}\n",
      "{'loss': 0.27812853, 'acc': 0.90077782, 'grad_norm': 7.73370314, 'learning_rate': 2.817e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.248047, 'epoch': 4.64, 'global_step/max_steps': '255/385', 'percentage': '66.23%', 'elapsed_time': '17m 7s', 'remaining_time': '8m 43s'}\n",
      "{'loss': 0.42952046, 'acc': 0.86251163, 'grad_norm': 11.96085739, 'learning_rate': 2.625e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.250413, 'epoch': 4.73, 'global_step/max_steps': '260/385', 'percentage': '67.53%', 'elapsed_time': '17m 18s', 'remaining_time': '8m 19s'}\n",
      "{'loss': 0.35174856, 'acc': 0.88666449, 'grad_norm': 8.55393028, 'learning_rate': 2.438e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.250409, 'epoch': 4.82, 'global_step/max_steps': '265/385', 'percentage': '68.83%', 'elapsed_time': '17m 38s', 'remaining_time': '7m 59s'}\n",
      "{'loss': 0.35832195, 'acc': 0.88696852, 'grad_norm': 7.03528786, 'learning_rate': 2.256e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.252023, 'epoch': 4.91, 'global_step/max_steps': '270/385', 'percentage': '70.13%', 'elapsed_time': '17m 51s', 'remaining_time': '7m 36s'}\n",
      "{'loss': 0.39782383, 'acc': 0.87252884, 'grad_norm': 12.17825508, 'learning_rate': 2.079e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.252912, 'epoch': 5.0, 'global_step/max_steps': '275/385', 'percentage': '71.43%', 'elapsed_time': '18m 7s', 'remaining_time': '7m 14s'}\n",
      "{'loss': 0.35216651, 'acc': 0.8915246, 'grad_norm': 10.04903603, 'learning_rate': 1.907e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.25331, 'epoch': 5.09, 'global_step/max_steps': '280/385', 'percentage': '72.73%', 'elapsed_time': '18m 25s', 'remaining_time': '6m 54s'}\n",
      "{'loss': 0.28929186, 'acc': 0.91684971, 'grad_norm': 7.0113368, 'learning_rate': 1.741e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.253331, 'epoch': 5.18, 'global_step/max_steps': '285/385', 'percentage': '74.03%', 'elapsed_time': '18m 44s', 'remaining_time': '6m 34s'}\n",
      "{'loss': 0.37409654, 'acc': 0.88714046, 'grad_norm': 9.95090294, 'learning_rate': 1.58e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.255452, 'epoch': 5.27, 'global_step/max_steps': '290/385', 'percentage': '75.32%', 'elapsed_time': '18m 55s', 'remaining_time': '6m 11s'}\n",
      "{'loss': 0.24848876, 'acc': 0.91320448, 'grad_norm': 7.3790288, 'learning_rate': 1.427e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.256804, 'epoch': 5.36, 'global_step/max_steps': '295/385', 'percentage': '76.62%', 'elapsed_time': '19m 8s', 'remaining_time': '5m 50s'}\n",
      "{'loss': 0.31651657, 'acc': 0.89571228, 'grad_norm': 9.04441071, 'learning_rate': 1.279e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.257407, 'epoch': 5.45, 'global_step/max_steps': '300/385', 'percentage': '77.92%', 'elapsed_time': '19m 25s', 'remaining_time': '5m 30s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa2921aa0d84c2b8b9a295083ccd726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|                                                                                      | 0/55 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828\\checkpoint-300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3057037, 'eval_acc': 0.90719385, 'eval_runtime': 50.1295, 'eval_samples_per_second': 17.395, 'eval_steps_per_second': 1.097, 'epoch': 5.45, 'global_step/max_steps': '300/385', 'percentage': '77.92%', 'elapsed_time': '20m 15s', 'remaining_time': '5m 44s'}\n",
      "{'loss': 0.42676053, 'acc': 0.88343782, 'grad_norm': 7.69617653, 'learning_rate': 1.139e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.248919, 'epoch': 5.55, 'global_step/max_steps': '305/385', 'percentage': '79.22%', 'elapsed_time': '20m 25s', 'remaining_time': '5m 21s'}\n",
      "{'loss': 0.29919531, 'acc': 0.91252623, 'grad_norm': 7.81879854, 'learning_rate': 1.006e-05, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.250924, 'epoch': 5.64, 'global_step/max_steps': '310/385', 'percentage': '80.52%', 'elapsed_time': '20m 35s', 'remaining_time': '4m 58s'}\n",
      "{'loss': 0.30290046, 'acc': 0.91132517, 'grad_norm': 7.45170736, 'learning_rate': 8.8e-06, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.251513, 'epoch': 5.73, 'global_step/max_steps': '315/385', 'percentage': '81.82%', 'elapsed_time': '20m 52s', 'remaining_time': '4m 38s'}\n",
      "{'loss': 0.3167891, 'acc': 0.90487823, 'grad_norm': 8.00732708, 'learning_rate': 7.62e-06, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.25422, 'epoch': 5.82, 'global_step/max_steps': '320/385', 'percentage': '83.12%', 'elapsed_time': '20m 58s', 'remaining_time': '4m 15s'}\n",
      "{'loss': 0.35086179, 'acc': 0.87491636, 'grad_norm': 9.9865427, 'learning_rate': 6.52e-06, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.253782, 'epoch': 5.91, 'global_step/max_steps': '325/385', 'percentage': '84.42%', 'elapsed_time': '21m 20s', 'remaining_time': '3m 56s'}\n",
      "{'loss': 0.3016047, 'acc': 0.90998745, 'grad_norm': 9.789711, 'learning_rate': 5.5e-06, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.253993, 'epoch': 6.0, 'global_step/max_steps': '330/385', 'percentage': '85.71%', 'elapsed_time': '21m 39s', 'remaining_time': '3m 36s'}\n",
      "{'loss': 0.21037869, 'acc': 0.92669487, 'grad_norm': 5.34359646, 'learning_rate': 4.56e-06, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.255672, 'epoch': 6.09, 'global_step/max_steps': '335/385', 'percentage': '87.01%', 'elapsed_time': '21m 50s', 'remaining_time': '3m 15s'}\n",
      "{'loss': 0.28812971, 'acc': 0.91179714, 'grad_norm': 6.02059603, 'learning_rate': 3.7e-06, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.256862, 'epoch': 6.18, 'global_step/max_steps': '340/385', 'percentage': '88.31%', 'elapsed_time': '22m 3s', 'remaining_time': '2m 55s'}\n",
      "{'loss': 0.31084197, 'acc': 0.91966829, 'grad_norm': 8.17992306, 'learning_rate': 2.93e-06, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.257336, 'epoch': 6.27, 'global_step/max_steps': '345/385', 'percentage': '89.61%', 'elapsed_time': '22m 20s', 'remaining_time': '2m 35s'}\n",
      "{'loss': 0.29541001, 'acc': 0.91929293, 'grad_norm': 6.8134923, 'learning_rate': 2.25e-06, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.258261, 'epoch': 6.36, 'global_step/max_steps': '350/385', 'percentage': '90.91%', 'elapsed_time': '22m 35s', 'remaining_time': '2m 15s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e82b71297db40679177c50f74dc5ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|                                                                                      | 0/55 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828\\checkpoint-350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29678956, 'eval_acc': 0.9092074, 'eval_runtime': 51.3471, 'eval_samples_per_second': 16.982, 'eval_steps_per_second': 1.071, 'epoch': 6.36, 'global_step/max_steps': '350/385', 'percentage': '90.91%', 'elapsed_time': '23m 26s', 'remaining_time': '2m 20s'}\n",
      "{'loss': 0.35004208, 'acc': 0.89000597, 'grad_norm': 8.91835785, 'learning_rate': 1.66e-06, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.249722, 'epoch': 6.45, 'global_step/max_steps': '355/385', 'percentage': '92.21%', 'elapsed_time': '23m 41s', 'remaining_time': '2m 0s'}\n",
      "{'loss': 0.25232456, 'acc': 0.90258465, 'grad_norm': 6.05313921, 'learning_rate': 1.15e-06, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.251953, 'epoch': 6.55, 'global_step/max_steps': '360/385', 'percentage': '93.51%', 'elapsed_time': '23m 48s', 'remaining_time': '1m 39s'}\n",
      "{'loss': 0.26941168, 'acc': 0.91579742, 'grad_norm': 4.5447011, 'learning_rate': 7.4e-07, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.25222, 'epoch': 6.64, 'global_step/max_steps': '365/385', 'percentage': '94.81%', 'elapsed_time': '24m 6s', 'remaining_time': '1m 19s'}\n",
      "{'loss': 0.29722016, 'acc': 0.90866327, 'grad_norm': 8.74890614, 'learning_rate': 4.2e-07, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.253852, 'epoch': 6.73, 'global_step/max_steps': '370/385', 'percentage': '96.10%', 'elapsed_time': '24m 17s', 'remaining_time': '59s'}\n",
      "{'loss': 0.33805594, 'acc': 0.90511284, 'grad_norm': 10.22574043, 'learning_rate': 1.9e-07, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.254426, 'epoch': 6.82, 'global_step/max_steps': '375/385', 'percentage': '97.40%', 'elapsed_time': '24m 33s', 'remaining_time': '39s'}\n",
      "{'loss': 0.31705153, 'acc': 0.90054789, 'grad_norm': 10.95191956, 'learning_rate': 5e-08, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.256021, 'epoch': 6.91, 'global_step/max_steps': '380/385', 'percentage': '98.70%', 'elapsed_time': '24m 44s', 'remaining_time': '19s'}\n",
      "{'loss': 0.46042385, 'acc': 0.86851406, 'grad_norm': 20.99083328, 'learning_rate': 0.0, 'memory(GiB)': 16.12, 'train_speed(iter/s)': 0.255766, 'epoch': 7.0, 'global_step/max_steps': '385/385', 'percentage': '100.00%', 'elapsed_time': '25m 5s', 'remaining_time': '0s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a0fdaca5f342f7b029f6955a9f96b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|                                                                                      | 0/55 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Saving model checkpoint to C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828\\checkpoint-385\n",
      "[INFO:swift] last_model_checkpoint: C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828\\checkpoint-385\n",
      "[INFO:swift] best_model_checkpoint: C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828\\checkpoint-350\n",
      "[INFO:swift] images_dir: C:\\Users\\IScream\\Desktop\\study\\自然语言处理\\PJ-基于LLM的命名实体识别和摘要任务\\data\\output\\qwen2_5-0_5b-instruct\\v20-20241117-132828\\images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29697388, 'eval_acc': 0.9088413, 'eval_runtime': 50.9888, 'eval_samples_per_second': 17.102, 'eval_steps_per_second': 1.079, 'epoch': 7.0, 'global_step/max_steps': '385/385', 'percentage': '100.00%', 'elapsed_time': '25m 56s', 'remaining_time': '0s'}\n",
      "{'train_runtime': 1556.1636, 'train_samples_per_second': 3.922, 'train_steps_per_second': 0.247, 'train_loss': 0.5811025, 'epoch': 7.0, 'global_step/max_steps': '385/385', 'percentage': '100.00%', 'elapsed_time': '25m 56s', 'remaining_time': '0s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] End time of running main: 2024-11-17 13:54:33.951422\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from swift.llm import (\n",
    "    get_model_tokenizer, get_template, inference, ModelType, get_default_template_type,\n",
    "    DatasetName, InferArguments, SftArguments,\n",
    "    infer_main, sft_main, app_ui_main\n",
    ")\n",
    "from swift.tuners import Swift\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# 将CoNLL格式的文件转换为按类型分开的JSONL格式数据\n",
    "def convert_conll_to_separate_prompt_jsonl(conll_file, jsonl_file):\n",
    "    data = []\n",
    "    with open(conll_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    sentence = []\n",
    "    labels = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if sentence:\n",
    "                text = ''.join(sentence)\n",
    "                entities_by_type = extract_entities_by_type(sentence, labels)\n",
    "                samples = construct_separate_prompt_samples(text, entities_by_type)\n",
    "                data.extend(samples)\n",
    "                sentence = []\n",
    "                labels = []\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                word, label = line.split()\n",
    "                sentence.append(word)\n",
    "                labels.append(label)\n",
    "            except ValueError:\n",
    "                continue  # 跳过格式不正确的行\n",
    "\n",
    "    # 处理最后一个句子\n",
    "    if sentence:\n",
    "        text = ''.join(sentence)\n",
    "        entities_by_type = extract_entities_by_type(sentence, labels)\n",
    "        samples = construct_separate_prompt_samples(text, entities_by_type)\n",
    "        data.extend(samples)\n",
    "\n",
    "    # 保存为 JSONL 格式\n",
    "    with open(jsonl_file, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            json_line = json.dumps(item, ensure_ascii=False)\n",
    "            f.write(json_line + '\\n')\n",
    "\n",
    "# 提取句子中的实体并按类型分类\n",
    "def extract_entities_by_type(sentence, labels):\n",
    "    entities = []\n",
    "    entity = ''\n",
    "    entity_type = ''\n",
    "    for word, label in zip(sentence, labels):\n",
    "        if label.startswith('B-'):\n",
    "            if entity:  # 保存之前的实体\n",
    "                entities.append({'entity': entity, 'type': entity_type})\n",
    "            entity_type = label[2:]  # 提取实体类型\n",
    "            entity = word\n",
    "        elif label.startswith('I-') or label.startswith('E-'):\n",
    "            entity += word  # 拼接多字词的实体\n",
    "        elif label.startswith('S-'):\n",
    "            entities.append({'entity': word, 'type': label[2:]})  # 单字实体\n",
    "        else:  # 'O' 表示非实体\n",
    "            if entity:\n",
    "                entities.append({'entity': entity, 'type': entity_type})\n",
    "                entity = ''\n",
    "                entity_type = ''\n",
    "    # 添加最后一个实体\n",
    "    if entity:\n",
    "        entities.append({'entity': entity, 'type': entity_type})\n",
    "    \n",
    "    # 按类型分类实体\n",
    "    entities_by_type = {}\n",
    "    for ent in entities:\n",
    "        ent_type = ent['type']\n",
    "        entities_by_type.setdefault(ent_type, []).append(ent['entity'])\n",
    "    return entities_by_type\n",
    "\n",
    "# 构建按类型分开的Prompt样本\n",
    "def construct_separate_prompt_samples(text, entities_by_type):\n",
    "    # 定义实体类型及其对应的中文名称和指令模板\n",
    "    entity_definitions = [\n",
    "        {\"type\": \"PER\", \"name\": \"人名\", \"instruction\": \"请从以下句子中识别并提取出所有的人名，使用'，'分割，如果不存在则返回'null'。\"},\n",
    "        {\"type\": \"LOC\", \"name\": \"地名\", \"instruction\": \"请从以下句子中识别并提取出所有的地名，使用'，'分割，如果不存在则返回'null'。\"},\n",
    "        {\"type\": \"OFI\", \"name\": \"官职名\", \"instruction\": \"请从以下句子中识别并提取出所有的官职名，使用'，'分割，如果不存在则返回'null'。\"},\n",
    "        {\"type\": \"BOOK\", \"name\": \"书名\", \"instruction\": \"请从以下句子中识别并提取出所有的书名，使用'，'分割，如果不存在则返回'null'。\"}\n",
    "        # 根据需要还添加更多实体类型的指令\n",
    "    ]\n",
    "\n",
    "    samples = []\n",
    "    for definition in entity_definitions:\n",
    "        ent_type = definition[\"type\"]\n",
    "        instruction = definition[\"instruction\"]\n",
    "        entities = entities_by_type.get(ent_type, [])\n",
    "        output_text = '，'.join(entities) if entities else \"null\"\n",
    "        sample = {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": text,\n",
    "            \"output\": output_text\n",
    "        }\n",
    "        samples.append(sample)\n",
    "    return samples\n",
    "\n",
    "# 处理多个CoNLL格式文件并转换为JSONL文件\n",
    "def process_files(conll_files, jsonl_files):\n",
    "    for conll_file, jsonl_file in zip(conll_files, jsonl_files):\n",
    "        convert_conll_to_separate_prompt_jsonl(conll_file, jsonl_file)\n",
    "\n",
    "# 定义训练集和验证集路径\n",
    "conll_train = './train.txt'\n",
    "conll_dev = './dev.txt'\n",
    "jsonl_train = './train_separate_prompts.jsonl'\n",
    "jsonl_dev = './dev_separate_prompts.jsonl'\n",
    "\n",
    "# 执行文件转换\n",
    "process_files(\n",
    "    conll_files=[conll_train, conll_dev],\n",
    "    jsonl_files=[jsonl_train, jsonl_dev]\n",
    ")\n",
    "\n",
    "# 指定模型类型和路径\n",
    "model_type = ModelType.qwen2_5_0_5b_instruct \n",
    "\n",
    "# 配置微调参数\n",
    "sft_args = SftArguments(\n",
    "    model_type=model_type,\n",
    "    dataset=['./train_separate_prompts.jsonl'],  # 数据集路径\n",
    "    val_dataset=['./dev_separate_prompts.jsonl'],  # 验证集路径\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=7,\n",
    "    output_dir='output',  # 输出目录\n",
    ")\n",
    "\n",
    "# 执行微调\n",
    "result = sft_main(sft_args)\n",
    "\n",
    "# 输出最佳模型路径\n",
    "best_model_checkpoint = result['best_model_checkpoint']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 模型推理\n",
    "\n",
    "使用微调后的模型对测试数据进行推理，主要步骤包括：\n",
    "\n",
    "1. **加载测试数据**：读取原始 NER 数据文件 `ner.txt`，按句子分割，并存储句子及其对应的标签。\n",
    "2. **加载微调模型**：通过 `Swift.from_pretrained` 加载最佳模型检查点，准备进行推理。\n",
    "3. **定义实体任务**：明确需要识别的实体类型及其对应的字段名，包括人名（`per`）、地名（`loc`）、官职名（`ofi`）和书名（`book`）。\n",
    "4. **构造并发送查询**：对于每个句子，针对每种实体类型构造相应的指令，并调用 `inference` 函数获取模型的响应。\n",
    "5. **保存结果**：将所有推理结果保存到 `result.json` 文件中。\n",
    "\n",
    "通过以上步骤，测试数据中的命名实体将被准确识别并保存，便于后续的分析与应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Downloading the model from ModelScope Hub, model_id: qwen/Qwen2.5-0.5B-Instruct\n",
      "[WARNING:modelscope] Using branch: master as version is unstable, use with caution\n",
      "[INFO:swift] Loading the model using model_dir: C:\\Users\\IScream\\.cache\\modelscope\\hub\\qwen\\Qwen2___5-0___5B-Instruct\n",
      "[INFO:swift] Setting torch_dtype: torch.bfloat16\n",
      "[INFO:swift] model_kwargs: {'device_map': 'cuda'}\n",
      "[INFO:swift] model.max_model_len: 32768\n",
      "testing:   0%|                                                                                 | 0/218 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "testing: 100%|███████████████████████████████████████████████████████████████████████| 218/218 [05:24<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推理完成,结果已保存到 result.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载NER数据\n",
    "ner_data = []\n",
    "sentence = []\n",
    "labels = []\n",
    "\n",
    "# 打开NER文件，并逐行读取数据\n",
    "with open('./ner.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        # 遇到空行，保存当前句子及标签，并重置\n",
    "        if not line:\n",
    "            if sentence and labels:\n",
    "                ner_data.append((sentence, labels))\n",
    "                sentence = []\n",
    "                labels = []\n",
    "        else:\n",
    "            # 将每行的单词和标签分开，并存入对应列表\n",
    "            word, label = line.split('\\t')\n",
    "            sentence.append(word)\n",
    "            labels.append(label)\n",
    "\n",
    "    # 防止文件末尾无空行的情况，添加最后一个句子及标签\n",
    "    if sentence and labels:\n",
    "        ner_data.append((sentence, labels))\n",
    "\n",
    "# 加载微调模型的检查点目录\n",
    "ckpt_dir = best_model_checkpoint  # 从之前的微调结果中获取\n",
    "model_type = ModelType.qwen2_5_0_5b_instruct  # 指定模型类型\n",
    "template_type = get_default_template_type(model_type)  # 获取默认模板类型\n",
    "\n",
    "# 初始化模型和分词器\n",
    "model_id_or_path = None  # 此处为None，表示加载本地模型\n",
    "model, tokenizer = get_model_tokenizer(\n",
    "    model_type, \n",
    "    model_id_or_path=model_id_or_path, \n",
    "    model_kwargs={'device_map': 'cuda'}\n",
    ")\n",
    "\n",
    "# 设置生成配置的最大生成token长度\n",
    "model.generation_config.max_new_tokens = 128\n",
    "\n",
    "# 存储推理结果\n",
    "results = []\n",
    "\n",
    "# 从检查点加载微调后的模型\n",
    "model = Swift.from_pretrained(model, ckpt_dir, inference_mode=True)\n",
    "\n",
    "# 获取推理时使用的模板\n",
    "template = get_template(template_type, tokenizer)\n",
    "\n",
    "# 定义NER任务中实体类型及其对应的字段名\n",
    "ner_tasks = [\n",
    "    (\"人名\", \"per\"),  # 人名对应字段名 \"per\"\n",
    "    (\"地名\", \"loc\"),  # 地名对应字段名 \"loc\"\n",
    "    (\"官职名\", \"ofi\"),  # 官职名对应字段名 \"ofi\"\n",
    "    (\"书名\", \"book\")  # 书名对应字段名 \"book\"\n",
    "]\n",
    "\n",
    "# 定义保存结果的路径\n",
    "save_path = 'result.json'\n",
    "\n",
    "# 遍历每个句子和标签进行推理\n",
    "for idx, data in tqdm(enumerate(ner_data), desc=\"testing\", total=len(ner_data)):\n",
    "    sentence, labels = data  # 分别获取句子和标签\n",
    "    sentence_str = ''.join(sentence)  # 将句子列表拼接成字符串\n",
    "    result_entry = {'sentence': sentence_str}  # 初始化结果字典\n",
    "\n",
    "    # 针对每种实体类型进行推理\n",
    "    for entity_type, key in ner_tasks:\n",
    "        # 构造查询指令，指定实体类型\n",
    "        query = f\"请从以下句子中识别并提取出所有的{entity_type}，使用'，'分割，如果不存在则返回'null'。\\n{sentence_str}\"\n",
    "        response, history = inference(model, template, query)  # 推理生成结果\n",
    "        result_entry[key] = response  # 将推理结果存入字典中\n",
    "    \n",
    "    # 将当前句子的结果添加到结果列表中\n",
    "    results.append(result_entry)\n",
    "\n",
    "with open(save_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"推理完成, 结果已保存到 {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 模型评估\n",
    "\n",
    "评估过程主要通过计算 F1-macro 得分来衡量模型在命名实体识别任务上的性能。主要步骤包括：\n",
    "\n",
    "1. **收集真实标签**：从测试数据中提取所有真实的实体标签，存储在 `y_true` 列表中。\n",
    "2. **处理预测结果**：解析模型的预测结果，将其转换为与真实标签格式一致的预测标签，存储在 `y_pred` 列表中。\n",
    "3. **计算 F1-macro**：使用 `sklearn` 库中的 `f1_score` 函数，计算 F1-macro 得分。\n",
    "\n",
    "通过以上步骤，可以有效评估模型在命名实体识别任务上的性能，反映模型在各类实体上的综合表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "命名实体标注任务评估结果：\n",
      "F1 macro score: 0.6876\n"
     ]
    }
   ],
   "source": [
    "# 加载模型预测结果\n",
    "with open('result.json', 'r', encoding='utf-8') as f:\n",
    "    final_result = json.load(f)  # 加载模型预测结果文件，结果是一个JSON数组\n",
    "\n",
    "# 真实标签（y_true）和预测标签（y_pred）的列表\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# 从NER数据中提取真实标签，逐个句子展开\n",
    "for _, labels in ner_data:\n",
    "    y_true.extend(labels)  # 将所有句子的真实标签按顺序追加到 y_true 中\n",
    "\n",
    "# 定义实体类型及其对应的标签前缀\n",
    "entity_types = {\n",
    "    \"per\": \"PER\",  # 人名\n",
    "    \"loc\": \"LOC\",  # 地名\n",
    "    \"ofi\": \"OFI\",  # 官职名\n",
    "    \"book\": \"BOOK\"  # 书名\n",
    "}\n",
    "\n",
    "# 辅助函数：标注预测实体到标签列表\n",
    "def mark_entities(sentence, entities, tag_prefix, pred_labels):\n",
    "    if not entities or entities.strip() == 'null':\n",
    "        return  # 如果实体为空或为 'null'，直接返回\n",
    "\n",
    "    for entity in entities.split('，'):  # 遍历所有实体\n",
    "        entity = entity.strip()\n",
    "        if entity == 'null' or not entity:\n",
    "            continue  # 跳过无效实体\n",
    "        start = sentence.find(entity)  # 查找实体在句子中的起始位置\n",
    "        if start == -1:\n",
    "            continue  # 如果实体不在句子中，跳过\n",
    "        end = start + len(entity)  # 计算实体的结束位置\n",
    "        if len(entity) == 1:\n",
    "            # 单字实体标注为 S\n",
    "            pred_labels[start] = f'S-{tag_prefix}'\n",
    "        else:\n",
    "            # 多字实体，起始标注为 B，结束标注为 E，中间标注为 I\n",
    "            pred_labels[start] = f'B-{tag_prefix}'\n",
    "            for i in range(start + 1, end - 1):\n",
    "                pred_labels[i] = f'I-{tag_prefix}'\n",
    "            pred_labels[end - 1] = f'E-{tag_prefix}'\n",
    "\n",
    "# 遍历模型预测结果，生成预测标签\n",
    "for item in final_result:\n",
    "    sentence = item['sentence']  # 获取句子内容\n",
    "    pred_labels = [\"O\"] * len(sentence)  # 初始化预测标签为 \"O\"（非实体）\n",
    "\n",
    "    # 遍历所有实体类型，标注对应的实体\n",
    "    for key, tag_prefix in entity_types.items():\n",
    "        entities = item.get(key, 'null')  # 获取当前实体类型的预测结果\n",
    "        mark_entities(sentence, entities, tag_prefix, pred_labels)  # 标注实体到预测标签列表\n",
    "\n",
    "    # 将预测标签追加到 y_pred 列表中\n",
    "    y_pred.extend(pred_labels)\n",
    "\n",
    "# 计算 F1 macro平均分数\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "print(f\"命名实体标注任务评估结果：\")\n",
    "print(f\"F1 macro score: {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 结果分析\n",
    "\n",
    "通过上述实验，基于LLM的方法在命名实体识别任务上达到了F1-macro得分为0.6876，超过了满分标准，说明识别效果较好。\n",
    "\n",
    "实验过程中展示出LLM在执行命名实体识别任务时有以下优劣势：\n",
    "### 优势：\n",
    "\n",
    "- **灵活性高**：通过设计不同的提示，LLM能够适应多种实体类型的识别任务。\n",
    "- **少量样本效果良好**：在训练数据较少的情况下，LLM依然能够通过预训练知识进行有效的实体识别。\n",
    "- **易于扩展**：添加新的实体类型只需设计相应的提示，无需修改模型架构。\n",
    "\n",
    "### 劣势：\n",
    "\n",
    "- **原始性能较差**：原始LLM受到其参数量等因素的限制，在NER任务中表现较差，必须对其进行预训练或微调等操作\n",
    "- **计算资源需求高**：LLM需要较大的计算资源，尤其是在推理阶段。\n",
    "- **整体性能有限**：相比于专门为NER任务设计的模型（如BiLSTM），LLM在整体F1-macro得分上表现略逊一筹。\n",
    "- **实体位置定位不精确**：由于LLM的生成式特性，可能存在实体位置定位不准确的问题，影响评估指标。\n",
    "\n",
    "\n",
    "## 1.6. 对比分析\n",
    "\n",
    "### 1.6.1 性能结果对比\n",
    "\n",
    "| 模型类型     | F1-macro 得分 |\n",
    "|--------------|----------------|\n",
    "| 基于LLM     | 0.6876         |\n",
    "| 基于BiLSTM  | 0.7111         |\n",
    "\n",
    "### 1.6.2 优劣势对比\n",
    "\n",
    "| 模型类型     | 优势                                                         | 劣势                                                         |\n",
    "|--------------|--------------------------------------------------------------|--------------------------------------------------------------|\n",
    "| 基于LLM     |  - 高灵活性，易于适应多种实体类型<br> - 能够利用预训练知识进行少量样本学习 |  - 计算资源需求高<br>- 整体性能略低于专用模型<br>- 实体定位不够精确 |\n",
    "| 基于BiLSTM  |  - 更高的F1-macro得分<br> - 精确的实体位置定位<br> - 计算资源需求较低 |  - 适应性相对较低，需针对特定任务设计架构<br>- 扩展到更多实体类型需重新训练 |\n",
    "\n",
    "### 1.6.3 综合分析\n",
    "\n",
    "基于BiLSTM的NER模型在本实验中表现出更高的F1-macro得分，主要因为其架构专门针对序列标注任务设计，能够更精确地捕捉词语间的依赖关系和实体边界。然而，基于LLM的方法展现出更高的灵活性和适应性，在处理复杂指令和多任务学习方面具有显著优势。\n",
    "\n",
    "## 1.7. 总结\n",
    "通过对基于LLM的命名实体识别任务结果进行分析：\n",
    "\n",
    "- LLM模型在命名实体识别任务中表现出较强的语言理解和生成能力，在F1-macro上表现优异。最终超过了**满分标准**：\"达到0.6以上\"\n",
    "- 尽管LLM在最终分数上比基于神经网络的模型略逊一筹，但其在处理复杂指令和多任务学习方面具有显著优势，适用于需要高度灵活性的应用场景。尤其在面对多样化的实体类型时，只需通过设计不同的提示即可完成任务，而无需重新设计模型架构。\n",
    "- 为了进一步提升命名实体识别的质量，未来工作可探索结合两种方法的优点，进一步提升NER任务的性能和应用范围。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 文本摘要任务\n",
    "## 2.1 设计Prompt并生成摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import re\n",
    "from rouge import Rouge\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# 设置镜像源\n",
    "os.environ[\"HF_HUB_BASE\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "# 加载模型和分词器\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(\"cuda\")  # 移动模型到GPU\n",
    "model.eval()  # 设置为评估模式\n",
    "\n",
    "# 设置填充和截断方向\n",
    "tokenizer.padding_side = \"left\"  # 设置左侧填充\n",
    "tokenizer.truncation_side = \"left\"  # 设置左侧截断\n",
    "\n",
    "# 加载并处理摘要数据集\n",
    "def load_summarization_data(filename):\n",
    "    articles = []\n",
    "    references = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            article = ''.join(data['article'])\n",
    "            summary = data['summary']\n",
    "            articles.append(article)\n",
    "            references.append(summary)\n",
    "    return articles, references\n",
    "\n",
    "sum_articles, sum_references = load_summarization_data('summary.jsonl')\n",
    "\n",
    "\n",
    "# 批量生成摘要函数\n",
    "def generate_summaries_in_batches(articles, references, batch_size=4, test=False, test_limit=10):\n",
    "    all_summaries = []\n",
    "    \n",
    "    # 定义提示模板\n",
    "    prompt_template = (\n",
    "        \"请阅读以下文章，并用50到100字对其进行总结，突出主要事件和关键人物，\"\n",
    "        \"不要引入原文中没有的事实。\\n\\n\"\n",
    "        \"文章：{article}\\n\"\n",
    "        \"摘要：\"\n",
    "    )\n",
    "    \n",
    "    printed = 0  # 记录已打印的摘要数量\n",
    "    \n",
    "    # 使用tqdm显示进度条\n",
    "    for i in tqdm(range(0, len(articles), batch_size), desc=\"Generating Summaries\"):\n",
    "        batch = articles[i:i + batch_size]\n",
    "        batch_refs = references[i:i + batch_size]\n",
    "        \n",
    "        prompts = [prompt_template.format(article=article) for article in batch]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=2048  # 根据模型支持的最大长度设置\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():  # 禁用梯度计算\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,        # 控制生成摘要的长度\n",
    "                num_beams=5,               # 增加束宽度以提高质量\n",
    "                early_stopping=True,       # 生成到达EOS token时停止\n",
    "                no_repeat_ngram_size=3,    # 防止重复\n",
    "                temperature=0.7,           # 控制生成多样性\n",
    "                top_p=0.95,                # 核心采样，增加多样性\n",
    "                top_k=50,                  # 前K采样\n",
    "                eos_token_id=tokenizer.eos_token_id,  # 设置结束标记\n",
    "                pad_token_id=tokenizer.pad_token_id,  # 设置填充标记\n",
    "            )\n",
    "        \n",
    "        summaries = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        \n",
    "        # 提取摘要部分，清理不必要的内容\n",
    "        extracted_summaries = []\n",
    "        for summary in summaries:\n",
    "            # 尝试提取“摘要：”后的内容\n",
    "            match = re.search(r'摘要[:：]\\s*(.*)', summary, re.DOTALL)\n",
    "            if match:\n",
    "                extracted_summary = match.group(1).strip()\n",
    "                # 只保留第一段，防止后续生成内容干扰\n",
    "                extracted_summary = extracted_summary.split('\\n')[0]\n",
    "                # 移除可能的后续指令或对话\n",
    "                extracted_summary = re.split(r'(Human:|Assistant:)', extracted_summary)[0].strip()\n",
    "                extracted_summaries.append(extracted_summary)\n",
    "            else:\n",
    "                # 如果找不到“摘要：”，则尝试从最后一段提取摘要\n",
    "                lines = summary.strip().split('\\n')\n",
    "                if lines:\n",
    "                    last_line = lines[-1].strip()\n",
    "                    extracted_summaries.append(last_line)\n",
    "                else:\n",
    "                    extracted_summaries.append(\"N/A\")\n",
    "        \n",
    "        all_summaries.extend(extracted_summaries)\n",
    "        \n",
    "        if test:\n",
    "            for j in range(len(batch)):\n",
    "                if printed >= test_limit:\n",
    "                    break\n",
    "                idx = i + j\n",
    "                if idx >= len(articles):\n",
    "                    break\n",
    "                print(f\"文章 {idx+1}：\")\n",
    "                print(batch[j])\n",
    "                print(\"\\n模型生成的摘要：\")\n",
    "                print(extracted_summaries[j])\n",
    "                print(\"\\n参考摘要：\")\n",
    "                print(batch_refs[j])\n",
    "                print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "                printed += 1\n",
    "            if printed >= test_limit:\n",
    "                break  # 达到测试限制后退出\n",
    "    return all_summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设计Prompt并优化\n",
    "\n",
    "在进行文本摘要生成时，设计合适的Prompt至关重要，它直接影响到模型的生成效果和最终的摘要质量。\n",
    "\n",
    "### 1. 设计初始Prompt\n",
    "\n",
    "初始的Prompt设计要简单明了，确保能引导模型理解任务并生成符合要求的摘要。我在设计时主要考虑了以下几个方面：\n",
    "\n",
    "- **明确任务要求**：Prompt明确指出要对文章进行总结，突出主要事件和人物。\n",
    "- **控制生成摘要的长度**：设置了“50到100字”的字数范围，引导模型生成简洁、精炼的摘要。\n",
    "- **格式化结构**：通过将文章和摘要分开，使用“文章：”与“摘要：”标识，有助于模型区分输入与生成内容的部分。\n",
    "\n",
    "### 2. 模型生成时的参数设置\n",
    "\n",
    "为了提高生成摘要的质量，我在生成摘要时使用了多个参数控制生成过程。具体如下：\n",
    "\n",
    "- `max_new_tokens=150`：控制生成摘要的最大长度，确保生成的内容不会过长。\n",
    "- `num_beams=5`：增加束搜索宽度（beam search），提高生成结果的多样性和质量。\n",
    "- `temperature=0.7` 和 `top_p=0.95`：这些参数控制生成的多样性。`temperature`较低时生成的内容更具确定性，而`top_p`限制了生成内容的范围，避免出现无关的或过于随机的内容。\n",
    "- `no_repeat_ngram_size=3`：防止重复生成相同的词组，提高摘要的流畅度和信息密度。\n",
    "- `early_stopping=True`：当模型生成结束标记时提前停止，确保摘要的完整性。\n",
    "\n",
    "这些参数共同作用，确保了生成的摘要既能准确提取文章的关键信息，又能避免过于冗长或重复的内容。\n",
    "\n",
    "### 3. 优化Prompt\n",
    "\n",
    "随着测试的进行，我逐渐发现生成摘要的质量有提升空间，特别是在摘要的简洁性和准确性上。除此之外，发现LLM由于幻觉在生成的摘要中产生了很多原文中没有的事实。为此，我对Prompt进行了以下优化：\n",
    "\n",
    "- **明确字数限制**：原始的Prompt要求生成摘要控制在50到100字之间，但模型的生成结果往往超过或不足。这时，我加强了字数范围的提示，明确告知模型生成不超过100字的摘要。\n",
    "- **减少冗余信息**：部分生成的摘要存在过多的背景信息或次要细节，这影响了摘要的简洁性。为了提高准确性和简洁度，我在Prompt中加入了对“去除不必要信息”的要求。\n",
    "- **避免引入原文中没有的事实**：明确告诉LLM不要虚构事实，由此尽量减少LLM的幻觉现象，提高摘要的准确性，提高分数。\n",
    "\n",
    "### 4. 后处理优化\n",
    "\n",
    "为了进一步提升生成摘要的质量，我对模型的输出进行了后处理：\n",
    "\n",
    "- **提取摘要部分**：由于模型生成的内容有时包含不必要的对话或指令，我使用正则表达式提取“摘要：”后的部分，并清理掉多余的内容。\n",
    "- **控制摘要格式**：对于生成摘要中可能包含的多余信息（如对话模式或不必要的细节），我通过字符串操作进行了清理，只保留摘要的核心内容。\n",
    "\n",
    "### 5. 优化总结\n",
    "\n",
    "通过设计合理的Prompt和调整生成参数，我能够有效地引导模型生成更精炼和准确的摘要。此外，通过后处理步骤，我进一步清理了模型生成的摘要内容，去除了冗余部分，确保最终摘要的质量。这些设计和优化大大提高了生成摘要的准确度，并且使摘要更加符合实际需求。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 案例分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在进行初步测试...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:   0%|                                                                      | 0/5 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Generating Summaries:  20%|████████████▍                                                 | 1/5 [00:21<01:27, 21.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文章 1：\n",
      "福建 法治 报 - 海 峡 法治在线 6月 1日 讯 2013年 9月 , 吴某 和 罗某 等人 到 连城县 莲峰镇 某 休闲吧 包厢 喝酒 时 , 因 服务员 在 关 大厅 的 灯时 错 把 所有 灯 关掉 , 使得 其中 一个 朋友 起身 时 碰到 啤酒瓶 并 砸到 脚 。吴某 得知 后 将 包厢 内 的 桌子 掀翻 后 到 吧台 找 服务员 理论 , 一言不合 就 打砸 吧台 物品 , 并 拳打 服务员 周某 。随后 , 吴某 伙同 朋友 砸坏 了 包厢 内 的 菜盘 、 杯子 、 烟灰缸 等 财物 。在 休闲吧 老板 和 民警 到场 后 , 吴某 还 欲 对 休闲吧 老板 进行 殴打 , 被 民警 制止 。经 鉴定 , 被 损坏 财物 在 价值 为 2991 元 。近日 , 连城县 法院 依法 审结 了 此案 。( 李 泗 峰 )\n",
      "\n",
      "模型生成的摘要：\n",
      "吴某和罗某等人在连城县莲峰镇某休闲吧包厢喝酒时，因服务员在大堂关灯时把所有灯关掉，导致吴某朋友起身时碰倒酒瓶并砸到脚。吴某得知后，将包厢内桌椅翻转后到吧台找服务员理论，拳打服务员周某。随后，吴某伙同朋友砸坏包厢内的物品，包括盘子、杯子、烟灰缸等。在休闲吧老板和民警到场后，吴某一伙欲对老板进行殴打，被民警制止。经鉴定，被损坏财物价值为29,910元。近日，连城县人民法院依法审理\n",
      "\n",
      "参考摘要：\n",
      "[ 连城县 ] 服务员 失手 关灯 , 男子 冲动 之下 打砸 出气\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "文章 2：\n",
      "宫 宗良 和 姜翠 卿 在 拍 婚纱照 。为 让 患病 老伴 开心 , 他 和 她 一起 参加 补拍 婚纱照 活动 5月 11 日 上午 , 在 市区 悦海 公园里 , 有 5 对 身穿 婚纱 礼服 的 老人 格外 引人注目 。这些 已 年 过 花甲 的 老人 们 , 有几分 局促 几分 欣喜 也 有 几分 可爱 。原来 , 这 是 经 区 西苑 街道办事处 府 安 社区 和 威海 北洋 职业 技术学 校 的 志愿者 们 免费 为 老人 举办 “ 最美 夕阳红 。 大爱 府 安情 ” 拍 婚纱照 活动 的 现场 。活动 中 社区 工作人员 和 志愿者 们 不仅 免费 为 老人 们 精心 化妆 , 还 负责 后期 修 片 , 并 向 老人 赠送 一个 精美 的 照片 摆台 , 这 让 老人 们 喜笑颜开 。在 这些 老人 中间 , 今年 71 岁 的 姜翠 卿 和 68 岁 的 宫 宗良 之间 的 真情 故事 , 深深 打动 了 记者 。威海 晚报 记者 。贾文娟 。通讯员 。于英娜 。姚威 。图说 起 初相识 , 她 笑得 很 甜蜜 在 拍摄 婚纱照 的 现场 , 记者 看到 姜翠 卿 和宫 宗良 这 对 老夫妻 总是 坐在 一起 , 仿佛 彼此间 有 说不完 的话 。看到 他们 脸上 幸福 开朗 的 笑容 , 深入 了解 两 人 之间 的 故事 后 , 记者 才 知道 两位 老人 很 不容易 。68 岁 的 宫 宗良 是 乳山市 崖子镇 青山村 人 。20 岁 那年 , 经 媒人 介绍 认识了 隔壁村 的 姜翠 卿 。姜翠 卿 年轻 的 时候 是 村里 的 大 美人 , 第一次 见面 , 就 看 上 了 老实 稳重 的 宫 宗良 。“ 那时候 上山 劳动 种 庄稼 , 看 他 身体 结实 , 是 个 好 劳力 , 话 也 很少 , 就 知道 他 靠得住 。 ”回忆起 48 年前 初相识 的 情形 , 姜翠 卿 眯着眼 笑得 很 开心 。说起 她 的 病 , 他 眼泪 止不住 结婚 以后 , 两人 非常 勤劳 , 虽 经济 不 太 宽裕 , 小日子 过得 挺 甜蜜 。他们 先后 生 了 3 个 女儿 , 女儿 各自 成家 以后 , 两位 老人 来到 府 安 社区 居住 , 和 大 女儿 生活 在一起 。随着 年龄 增大 , 姜翠 卿 的 身体 却 一 天 不如 一天 , 大病 小病 不断 。5 年前 , 突然 被 检查 出来 患 了 尿毒症 。“ 这 一 辈子 我 没 流过 几回 泪 。 知道 她 患 尿毒症 的 那天 我 哭了 , 心里 有 说 不出 的 滋味 。 ”说起 那段 最 黑暗 的 时光 , 宫 宗良 忍不住 抹 起 了 眼泪 , 在 一边 的 妻子 则 悄悄 握住 了 他 的 手 。五年 来 , 是 两位 老人 过得 最 艰辛 的 五年 , 也是 最 甜蜜 的 五年 。“ 每个 星期 做 三次 透析 , 都是 老伴 陪我 去 。 女儿 和 女婿 非常 好 , 很 关心 我 , 但 我 不想 给 他们 添麻烦 , 他们 都 要 上班 , 还要 照顾 孩子 。 ”姜翠 卿 告诉 记者 , 那些 艰难 的 日子 , 都是 老伴 陪着她 。五年 时间 , 在家 人和 老伴 的 细心 照顾 下 , 姜翠 卿 的 身体 居然 渐渐 好转 了 , 腰 也 好了 不少 , 现在 能 自己 走路 了 。披上 婚纱 , 他们 笑得 很 灿烂 为了 让 患病 的 老伴 开心 , 一 辈子 不懂 浪漫 的 宫 宗良 , 报名 参加 了 社区 举办 的 免费 拍 婚纱照 活动 。“ 因为 从来 没有 给 老伴 送 过 礼物 , 结婚 的 时候 甚至 没有 留下 一张 黑白 合影 照片 , 一直 觉得 有些 缺憾 。 ”宫 宗良 老人 告诉 记者 , 他 这次 和 老伴 一起 来 拍 婚纱照 , 就是 想 弥补 这个 缺憾 。“ 我 心里 不知道 有 多 高兴 呢 。 看 女儿 结婚 的 时候 穿 婚纱 , 心里 很 羡慕 , 从来没 想 过 这辈子 还 能 穿上 婚纱 拍照 。 ”在 拍摄 现场 , 身穿 洁白 婚纱 的 姜翠 卿 一直 很 开心 。也许 , 在 她 心里 , 能 和 老伴 一起 拍 婚纱照 , 就是 最 浪漫 的 事 。“ 我 能 想到 最 浪漫 的 事 , 就是 和你 一起 慢慢 变老 ! ” 看着 两位 老人 在 摄影师 的 镜头 下 恩爱 开心 的 模样 , 记者 禁不住 想起 了 那 句 有名 的 歌词 , 就让 记者 用 这 两句话 来 祝福 这 一对 相亲相爱 、 相依为命 的 恩爱 老人 。\n",
      "\n",
      "模型生成的摘要：\n",
      "宫宗良和姜翠卿在市中心悦海公园参加婚礼婚纱照活动。5月11日上午，5对身着婚纱的老人格外引人注意。这些已年过花甲的老人中，有几分局促，几分欣喜，几分可爱。原来，这是区西苑街道办事处府安社区和威海北洋职业技术学校志愿者免费为老人举办“最美夕阳红大爱府安情”婚礼纱照活动现场。活动期间，社区工作人员和志愿者不只免费为老人们精心化妆，还负责后期修片，并向老人们赠送一张精美的照片摆台，让老人们喜笑开颜。在这些老人中间，今年71岁的姜翠\n",
      "\n",
      "参考摘要：\n",
      "[ 环翠区 ] 七旬 老人 与 妻子 补拍 婚纱照 , 妻子 身 患 尿毒症 老人 不离不弃 ( 图 )\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  40%|████████████████████████▊                                     | 2/5 [00:37<00:53, 17.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文章 3：\n",
      "扬州晚报 2月 4日 消息 , 2月 2日 深夜 11点 多 , 居住 在 江苏 扬州 石油 山庄 的 市民 顾 女士 才 停好 车 , 就 听到 旁边 车位 的 车子 发出 一阵 “ 咚咚咚 ” 的 敲 窗 声 , 仔细 一看 , 车内 竟 被 困 着 一 老 一小 两个人 。记者 2月 3日 了解到 , 车内 被困 的 两人 是 奶奶 和 孙女 , 因 开车 的 夫妻俩 忙着 抢红包 , 将 老人 和 孩子 丢 在 了 车里 。邻居 奇遇 深夜 听到 隔壁 车内 有人 敲 窗 “ 把 我 吓一跳 , 一看 车 里 有 两个人 。 ”2月 3日 清晨 , 顾 女士 告诉 记者 , 2月 2日 晚上 11点 多 , 她 开车 回家 停 在 楼下 时 , 被 临近 车位 发出 的 声音 惊 住 。顾 女士 说 , 隔壁 车位 上 的 车内 后侧 窗户 传来 “ 咚咚 ” 的 敲击声 , 一开始 她 还 以为 车内 有人 盗窃 , 可 仔细 一看 , 却是 一个 老人 和 一个 小女孩 在 车内 。“ 喊 她们 开门 , 她们 也 不 知道 怎么 开 。 ”顾 女士 说 , 贴着 车窗 , 能 看到 小女孩 在 哭 , 里面 的 老人 也 急 的 不得了 。看到 这种 情况 , 顾 女士 终于 弄明白 了 , 原来 老人 和 孩子 是 被 锁 在 车内 里 了 。因为 经常 有 朋友 来 家里 玩 , 偶尔 也 会 占用 邻居 车位 , 所以 顾 女士 知道 车子 是 哪 一 户 人家 的 。“ 按 了 门铃 , 夫妻俩 下来 , 将 车里 的 老人 和 孩子 放 了 出来 。 ”顾 女士 说 , 女孩 才 3岁 , 听说 已 在 车内 困 了 40 多分钟 , 老人 是 孩子 的 奶奶 。车主 讲述 忙着 抢红包 , 以为 人 下来 了 “ 快 过年了 , 把 孩子 奶奶 接 过来 一起 过年 。 ”杨 女士 说 , 孩子 奶奶 是 他们 从 外地 接 来 的 , 一路上 开 了 3 个 多 小时 , 她 坐在 车里 没事 就 忙着 抢红包 、 向 好友 讨 支付宝 的 各种 “ 福 ” 等 。 杨 女士 说 , 丈夫 也是 位 85后 , 平时 就 喜欢 玩 抢红包 的 游戏 , 一路上 开着车 红包 的 提醒 就 响 个 不停 , 车子 一 停下来 , 丈夫 就 盯着 手机 抢 个 不停 并且 自顾自 的 上 了 楼 , 并 随手 按 了 锁车 键 。 “ 我 当时 以为 孩子 由 她 奶奶 带 上楼 了 , 下车 后 也 忙着 抢红包 就 直接 上楼 去 了 。 ”杨 女士 说 , 一路 奔波 太累 , 到了 家中 , 她 就 和 丈夫 躺在床上 继续 抢红包 。采访 中 杨 女士 说 , 当时 只是 以为 孩子 奶奶 抱着 女儿 上楼 慢 一些 , 没想到 一路上 可能 太 劳累 了 , 奶奶 和 孙女 在 后座 睡着了 , 醒来 后 发现 车 已经 锁 了 , 又 不知道 怎么 开门 。好在 是 邻居 及时 发现 并 好心 跑 来 喊 他们 , 不然 后果 不堪设想 。\n",
      "\n",
      "模型生成的摘要：\n",
      "2020年2月2日11时30分左右，江苏省扬州市石山庄居民顾女士在回家途中，听到车窗外传来“咚咚咚”的敲门声，惊慌失措地打开车门，发现车内有一老人和一个小女孩被困，顾女士立即拨打110报警，并将老人和小女孩安全救出。2月3日清晨，杨女士接到顾女士的电话，得知顾女士被困车内40多分钟。杨女士当时以为孩子由奶奶抱着上楼，但没想到路上太累，奶奶和孙女在后座睡着了，醒来后发现车已锁门，无法开门。\n",
      "\n",
      "参考摘要：\n",
      "扬州 一对 夫妻 停车 后 忙 抢 手机 红包 , 将 3岁 女儿 和 老人 锁 车内 , 祖孙 俩 深夜 敲 窗 求救\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "文章 4：\n",
      "近日 , 一名 51 岁 的 中年 男子 因 和 家人 生气 酒后 割腕 自杀 , 所幸 因 抢救 及时 捡回 了 一条命 。7月 19 日 , 家住 卧龙区 陆营镇 某村 的 中年男子 李某 因 琐事 和 家人 生气 独自 喝 起 了 闷酒 。之后 , 越 想 越 生气 的 李某 拿起 菜刀 企图 割腕 自杀 。当 看到 鲜血 流出 来 后 , 因 害怕 李某 开始 大声 呼救 , 家人 赶紧 拨打 120 急救电话 。经 南阳 卧龙 医院 救治 , 目前 , 李某 已经 出院 回家 。卧龙区 潦河镇 某村 54 岁 的 张某 长期 嗜酒 , 和 家人 关系 不合 。因此 产生 了 悲观厌世 情绪 。几天 前 , 张某 趁 家人 不备 酒后 割喉 自杀 。当 卧龙 医院 急救 人员 赶到 现场 时 , 发现 张某 已 无 任何 生命 迹象 。( 王雪 。薛文 )\n",
      "\n",
      "模型生成的摘要：\n",
      "7月19日，中年男子李某因琐事与家人生气，独自喝闷酒后割腕自杀。7月20日，张某因长期酗酒和悲观厌世情绪，趁家人不备割喉自杀。这两起自杀事件均发生在卧龙镇陆营村。\n",
      "\n",
      "参考摘要：\n",
      "[ 卧龙区 ] 中年男子 和 家人 发生 矛盾 , 持 菜刀 割腕 自杀 , 看到 流血 害怕 大声 呼救\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  60%|█████████████████████████████████████▏                        | 3/5 [00:59<00:40, 20.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文章 5：\n",
      "本月 11 号 , 高通 官方 正式 宣布 了 最新 旗舰 处理 骁龙 821 , 从 命名 上 也 能 看出 , 它 是 骁龙 820 的 小幅 升级版 。骁龙 821 同样 内置 四颗 Koyo 架构 核心 , 两颗 高频 核心 主频 最高 2.4GHz , 两颗 低频 核心 主频 最高 2GHz , GPU 频率 为 650 MHz 。得益于 主频 提升 , 骁龙 821 的 性能 相比 骁龙 820 最高 有 10 % 的 提升 。在 骁龙 821 首发 机型 上 , 华硕 这次 抢 了 先 。8月份 , 华硕 ZenFone 3 。Deluxe 将 全球 首次 搭载 骁龙 821 上市 。目前 , 该机 的 详细 配置 已经 出现 在 数据库 , 跑 分 也 一同 流出 。据 了解 , 该机 配备 5 . 7 英寸 1080P Super 。AMOLED 显示屏 , 搭载 主频 高达 2.4GHz 的 骁龙 821 处理器 , 6GB RAM , 2300 万 摄像头 。安兔兔 评测 跑 分 分数 大概 在 14 . 8万 分 左右 。此外 , 该机 还 配备 了 2300 万 像素 摄像头 ( 0 . 03 秒 自动对焦 、 2 代 激光 、 光学防抖 、 RGB 色彩 校准 传感器 、 双 色温 闪光灯 ) 、 QC3.0 快充 ( 3000mAh 、 39 分钟 充 60 % ) 、 HiRes 金标 认证 ( 4 倍 CD 音质 )根据 此前 华硕 公布 的 消息 , 华硕 ZenFone 3 。Deluxe 还是 全球 首款 金属 一体机 身 但 采用 隐藏式 天线 设计 的 手机 , 华硕 达到 了 苹果 iPhone6S 都 没有 企及 的 新高度 。据悉 , 该机 将于 8月份 在 台湾 首发 上市 , 售价 24990 新台币 , 约合 人民币 5200 元 。\n",
      "\n",
      "模型生成的摘要：\n",
      "2018年8月11日，高通宣布了其最新旗舰处理器骁龙822的发布。这款处理器采用了四颗Kryo架构核心，两颗高频核心主频分别为2. 4GHz和2GHz，低频核心主频率最高为2GHz。GPU频率提升至6. 5GHz，性能提升10%。华硕ZenFone3. Deluxe版在8月份首次搭载了这款处理器。目前，该机的详细配置和跑分数据已经公开。该机配备了5. 7英寸10. 8英寸Super AMOLED屏幕，主频高达2400MHz的处理器，6GBRAM，23MP摄像头\n",
      "\n",
      "参考摘要：\n",
      "华硕 全球 首款 骁龙 821 手机 曝光 : 5 . 7 吋 屏 , 金属 一体机 身 , 隐藏式 天线 设计 , 5200 元 ( 图 )\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "文章 6：\n",
      "常小兵 曾 主事 中国联通 11年 , 是 中国联通 任期 最长 的 核心 高管 , 今年 8月 调任 中国 电信 董事长 。业内 普遍 认为 , 常小兵 案发 , 事 出 其 中国联通 任内 。《 财经 》 记者 。谢丽容 / 文 据 中国 电信 ( 0728 . HK ) 多位 人士 向 《 财经 》 记者 确认 , 中国 电信 董事长 常小兵 因 严重 违纪 被 带走 审查 。《 财经 》 记者 多次 拨打 常小兵 手机 , 均 处于 关机 状态 。中国 电信 原定 在 12月 28 日 召开 年度 工会 , 12月 26 日 晚上 , 中国 电信 各省 即将 赴京 参会 的 省级 高官 接到 会议 延期 消息 。常小兵 被 带走 调查 , 办公室 被 查封 的 消息 随后 开始 流传 。图说 : 中国 电信 今天 凌晨 流传 出来 的 一张 照片 , 显示 常小兵 的 办公室 已 被 查封 《 财经 》 记者 就此 向 中国 电信 新闻 宣传 部门 确认 , 未 得到 肯定 回应 。常小兵 曾 主事 中国联通 11年 , 今年 8月 , 时任 中国联通 董事长 的 常小兵 与 时任 中国 电信 董事长 王晓初 对调 , 出任 中国 电信 董事长 , 但 外界 普遍 认为 , 常小兵 被 立案 调查 , 事 出 其 在中国 联通 的 任内 。常小兵 今年 58 岁 , 1982 。年 毕业 于 南京 邮电 学院 电信 工程 系 , 1982 。年 起 , 历任 安徽省 六安 地区 邮电局 技术员 , 江苏省 南京市 电信局 、 江苏省 邮电 管理局 网管 中心 工程师 , 江苏省 邮电 管理局 电信 处 副处长 , 南京市 电信 局 副局长 。1996年 6月 之后 , 受 时任 邮电部 部长 ( 1998年 后 为 信息 产业部 部长 ) 吴基传 提拔 , 历任 中国 邮电 电信 总局 副局长 、 信息 产业部 电信 管理局 副局长 、 信息 产业部 电信 管理局 局长 2004年 11月 起 任 中国联通 公司 董事长 、 党组书记 2008年 电信 重组 后 , 任 联通 集团 与 网通 集团 合并 后 的 中国 联合 网络 通信 集团 董事长 兼 新 联通 筹备组 组长 , 后来 成为 新的 中国联通 董事长 。截至 今年 8月 , 常小兵 主事 中国联通 11年 , 是 中国联通 任期 最长 的 核心 高管 。常小兵 经历 了 中国联通 在 3G 时代 的 辉煌 和 4G 时代 的 失落 。多位 电信业 资深 人士 向 《 财经 》 记者 表示 , 联通 今天 的 成就 和 包袱 都 和 常小兵 分不开 , 整个 中国联通 深深 打 上 了 常小兵 的 烙印 。1998年 国务院 机构 改革 , 成立 信息 产业部 , 首任 信产部 部长 吴基传 大胆 提拔 了 一批 地市级 少壮派 高管 , 这批 名单 包括 了 常小兵 , 以及 原 中国移动 党组书记 、 副总经理 张春江 、 原 信息 产业部 总工程师 苏金 生 等 。当年 年底 , 中国 电信 业 开始 政企 分离 , 这 批 人 成为 电信 企业 的 第一批 核心 高管 。如果 消息 属实 , 常小兵 就是 这 批 人 中 第三位 落马 的 。张春江 因 受贿 746 万 于 2011年 被 法院 判处 死刑 , 缓期 二年 执行 , 2014 年 11月 减为 无期徒刑 ; 2011年 7月 , 工信部 原 总工程师 苏金 生因 收受 巨额 贿赂 , 生活 腐化堕落 , 被 开除党籍 和 公职 , 收缴 违纪 所得 , 并 移送 司法 机关 。2011年 11月 17 日 , 河北省 沧州市 中级 人民 法院 认定 苏金 生 犯 受贿罪 , 判处 有期徒刑 十三年 。2014 年 11月 27 日 至 12月 27 日 , 中央 第八 巡视组 对 中国 联通 进行 了 巡视 。中央 第八 巡视组 组长 宁 延 令 指出 , 中国联通 存在 一些 不容忽视 的 问题 , 主要 是 : 公司 党组 执行 党风 廉政 建设 责任制 不 到位 , 重 业务 轻 党建 , 纪检 监察 机构 履行 监督 责任 不力 。有的 领导 和 关键 岗位 人员 利用 职权 与 承包商 、 供应商 内外勾结 , 搞 权钱 、 权色 交易 ; 有的 纵容 支持 亲属 、 老乡 或 其他 关系 人 在 自己 管辖 范围 内 承揽 项目 或 开办 关联 企业 谋利 ; 干部 带病 提拔 、 选人 用人 不公 问题 反映 强烈 ; 投资 建设 、 物资 采购 领域 违规 问题 严重 。中央 巡视组 当时 还 收到 一些 联通 公司 领导 人员 的 问题 线索 , 并 按 有关 规定 转 中央 纪委 、 中央 组织部 、 国务院 国资委 等 有关 部门 处理 。相关 电信业 资深 人士 向 《 财经 》 记者 分析 , 常小兵 一案 恐 将 引发 电信业 新一轮 地震 。中国 电信 为 香港 上市公司 , 如果 常小兵 被 带走 调查 一事 属实 , 中国 电信 上市公司 将 在 近 日内 发布 公告 说明 情况 。\n",
      "\n",
      "模型生成的摘要：\n",
      "2020年8月11日，中国电信董事局主席、党组书记常少平因涉嫌严重违纪违法被调查。据《中国经营报》报道，常少平时事担任中国联通11年的核心高管，今年8月被调任中国电信董监高。业内普遍认为，这起事件可能牵涉到中国电信任期内的严重违规行为。中国电信原定于12月底召开年度工会，但因常小平被带走调查，会议延期至26日晚上举行。目前，中国信通院多位知情人士向《财经》记者确认，未得到任何官方回应。中国通信行业资深人士认为，这是中国电信在任期内的重大事件，\n",
      "\n",
      "参考摘要：\n",
      "媒体 称 中国 电信 董事长 常小兵 因 严重 违纪 被 带走 , 曾 执掌 中国联通 十余年 , 自称 薪水 每月 8 千 元\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  80%|█████████████████████████████████████████████████▌            | 4/5 [01:13<00:17, 17.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文章 7：\n",
      "[ 海峡 都市报 - 海峡网 ] 昨日 省 气象 部门 消息 , 今日 白天 , 南平市 多云到阴 有 阵雨 或 雷阵雨 , 部分 中雨 , 局部 大雨 到 暴雨 ; 其余 各市 多云 , 部分 有 阵雨 或 雷阵雨 , 局部 有 中到大雨 。雷雨时 伴有 强 雷电 、 短时 强降水 和 7-9 级 雷雨 大风 等 强对流 天气 。15日 夜间 到 16日 白天 , 南平 、 宁德 两市 和 三明市 北部 阴 有 阵雨 或 雷阵雨 , 部分 中到大雨 , 局部 暴雨 ; 其余 地区 多云到阴 , 有 阵雨 或 雷阵雨 , 局部 有 中到大雨 。16日 夜间 到 17日 白天 , 南平市 阴 有 中雨 , 伴有 雷电 , 部分 大雨 到 暴雨 ; 宁德 、 三明 两市 阴 有 阵雨 或 雷阵雨 , 部分 中雨 , 局部 大雨 到 暴雨 ; 其余 各市 多云到阴 , 有 阵雨 或 雷阵雨 , 局部 有 中到大雨 。( 记者 。徐昕 昀 )\n",
      "\n",
      "模型生成的摘要：\n",
      "2022年5月15日至17日期间，南平、宁德、三明等地出现强对流天气，包括雷雨、强雷电、短时强降水、大风和雷雨大风等气象灾害，给当地居民生活和生产造成严重影响。\n",
      "\n",
      "参考摘要：\n",
      "南平 今日 迎来 雷阵雨 , 局部 有 大雨 到 暴雨 , 预计 持续 到 明后天 , 请 注意 防范\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "文章 8：\n",
      "今天 下午 , 网友 担担面 0228 发 微博 称 自己 早上 “ 滴滴 ” 个 车 , 上车时 并没有 留意 到 司机 下身 的 异常 , 路上 无意间 看到 这 名 猥琐 司机 下身 仅 用 一条 五分裤 随意 覆盖 着 。车 为 红色 大众 , 车牌号 津 MM 61 xx 。途中 , 猥琐 司机 不仅 绕道 行驶 , 还 跟 @ 担担面 0228 聊 了 些 女性 话题 。@ 担担面 0228 发现 司机 的 猥琐 行为 后 , 果断 拍片 并 提前 下车 。随后 , @ 担担面 0228 联系 了 滴滴 客服 并 提供 了 相关 照片 。而 滴滴 方面 开始 仅仅 给出 了 10元 顺风车 券 作为 补偿 。之后 , @ 担担面 0228 又 在 微博上 晒 出 被 司机 威胁 的 截图 。如何 处置 “ 猥琐男 ” ?滴滴 方面 在 接受 新报 记者 采访 时 表示 , “ 我们 已经 对 此事 进行 了 调查 核实 , 也 与 乘客 进行 了 详细 沟通 , 并且 表达 了 慰问 和 歉意 。 目前 , 车主 多次 电话 联系 没人 接听 , 我们 将 继续 联系 核实 。 ”鉴于 目前 所 掌握 的 信息 , 车主 已经 涉嫌 违规 , 滴滴 方面 对于 车主 已经 进行 了 临时 封禁 处理 , 待 事情 进一步 核实 后 , 如果 确实 , 将会 对 车主 永久 封禁 。与 出租车 相比 , 顺风车 的 准入 门槛 更 低 。从 管理 角度 来看 , 出租车 受到 企业 、 客 管办 的 多重 直接 管理 , 顺风车 与 滴滴 公司 之间 更多 的 是 一种 合同 关系 。因此 , 对 司机 行为 的 约束力 更 弱 , 由此 带来 对 乘客 的 安全 隐患 也 会 更 大 。那么 , 对于 类似 问题 , 滴滴 方面 有 相关 的 管理 措施 吗 ?滴滴 公司 加强 对 快车 司机 准入 资质 的 审查 , 是 有 必要 的 , 这 在 一定 程度 上 可以 减少 意外 发生 的 风险 。但 提高 准入 资质 与 是否 会 发生 类似 意外 , 这 两者之间 并没有 必然 联系 , 需要 具体 问题 具体分析 。记者 了解到 , 按照 目前 滴滴 平台 的 管理 规范 , 任何 对于 他人 的 恶意 侵犯 和 攻击 都是 被 严厉 禁止 的 , 同时 谴责 这种 恶意 行为 , 并 呼吁 有 类似 遭遇 的 用户 第一时间 投诉 , 滴滴 方面 会 严查 处理 , 若 涉及 违法 , 也 将 协助 警方 调查 惩戒 。为 降低 安全 风险 , 滴滴 方面 研发 的 一键 报警 功能 已 在 实验 阶段 , 一旦 司机 和 乘客 在 行程 中 出现 意外 情况 , 可以 在 第一时间 向 警方 发送 报警 信息 。新报 新媒体 记者 。马雨彤 。\n",
      "\n",
      "模型生成的摘要：\n",
      "今天下午，网友担担面在微博上称自己早上“滴滴”个车，上车时没有留意到司机下身的异常，路途中无意间看到一名猥琐的司机仅用一条5分裤随意覆盖着。滴滴方面表示，已经对此次事件进行了调查核实，与乘客进行了沟通，并且表示会给予补偿。目前，司机多次电话联系滴滴客服，但未接听，滴滴方面将继续联系核实。对于类似问题，滴滴公司有相关管理措施吗？滴滴公司加强了对快车司机资质的审查，但提高资质与是否发生类似意外风险之间没有必然联系，需要具体问题具体分析。记者了解到，滴滴平台对恶意侵犯和攻击的行为进行严厉\n",
      "\n",
      "参考摘要：\n",
      "天津 滴滴 男 司机 下身 赤裸 载 女乘客 , 仅 盖 一条 短裤 , 聊 女性 话题 ; 客服 : 赔 10元 代金券 ( 图 )\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:  80%|█████████████████████████████████████████████████▌            | 4/5 [01:32<00:23, 23.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文章 9：\n",
      "五华县 气象台 于 09 月 07 日 15 时 53 分 将 暴雨 黄色 预警 升级 为 橙色 预警 , 请注意 加强 防御 。\n",
      "\n",
      "模型生成的摘要：\n",
      "2022年09月07日15时53分，五华气象台将暴雨黄色预警升级为橙色预警。请注意加强防御。\n",
      "\n",
      "参考摘要：\n",
      "五华县 气象台 发布 暴雨 橙色 预警 , 请注意 加强 防御 。\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "文章 10：\n",
      "余杭 辉 ( 左 ) 带着 锦旗 到 公交 二 公司 感谢 , 车队 代表 接受 了 锦旗 。今天 上午 , 25岁 的 余杭 辉 带着 锦旗 赶到 金华 市 公交 集团 运营 二 公司 , “ 多亏 了 蒋永 泉 师傅 , 要不 是 他 后果 不堪设想 。 ”暴雨 来袭 。一 瞬间 车子 淹没 在水中 余杭 辉 告诉 记者 , 事情 发生 在 6月 29 日 , 也就是 金华 暴雨 的 那天 。“ 早上 8 点 左右 我 开车 带着 老婆 、 小孩 还有我 妈 去 含香 的 菜场 买菜 。 ”因为 暴雨 几条 熟悉 的 道路 都 无法 通行 了 。于 是 他 选择 从 隔壁 的 塘雅镇 楼下 徐村 绕 到 含香 去 。可是 余杭 辉 万万 没想到 就是 这个 决定 , 差点 酿成大祸 。因为 这条 田埂 小路 旁 有 一条 小溪 。持续 的 暴雨 , 使得 溪水 很快 就 满 了 , 淹没 了 道路 。“ 当 我 开到 那个 路段 时 , 大水 只 没过 了 前 车轮 的 一半 。 后车轮 还 没 碰到 水 。 ”余杭 辉 说 , 他 下意识 地下 车 查看 情况 。“ 我 觉得 应该 过不去 了 , 于是 上车 准备 退回去 。 ”但 就在 一分钟 的 时间内 , 情况 出现 了 不可 控制 的 转变 。一 上车 , 车子 就 被 水波 往前 移动 了 一段 。“ 我 挂 倒 档 想要 退 , 结果 又 一波 水 , 又 将 车子 往前 推 了 一把 。 ”余杭 辉 告诉 浙江 新闻 客户端 记者 这种 感觉 就 像 是 河水 涨潮 了 一样 。于 是 他 立马 打开 车窗 和 车门 , 让 家人 赶紧 下车 。也就 在 这 一分钟 时间内 , 车子 的 车头 竟 被 大水 淹没 了 。“ 我 的 孩子 才 15 个 月 大 , 大家 一下子 就 慌了 。 ”听到 “ 救命 ” 声 。休息 在家 的 公交车 司机 下水 援救 好在 一个 老奶奶 看到 了 这 一切 , 立马 跑到 村里 去 叫 人 。公交车 司机 蒋勇 泉 就是 闻讯 来 的 第一个 人 。二话不说 跳下 了 水中 。“ 当时 他 游 了 过来 , 测 了 下水 深 , 然后 回家 找 了 个 大 脸盆 。 ”余杭 辉 说 他 的 孩子 就是 放在 脸盆 中 , 靠着 蒋 师傅 一点点 的 拖到 了 安全 地带 。余杭 辉 的 老妈 会 游泳 , 但 老婆 不会 。村民 们 又 给 他们 提供 了 绳子 和 救生圈 。余杭 辉 用 绳子 绑 在 救生圈 上 , 和 蒋 师傅 两人 一起 将 老婆 拉 到了 安全 地段 。“ 因为 孩子 实在太 小 , 当时 一门心思 都铺 在 了 他 的 身上 。 ”余杭 辉 说 , 加上 大水 冲掉 了 他 的 眼镜 , 近视 五六百 度 的 他 无法 将 这些 好心人 一一 记下 。“ 事发 第二天 , 我 凭着 印象 去 感谢 几位 帮助 过 我们 的 人 , 这时 村子 里 的 人 告诉 了 我 蒋 师傅 的 情况 。 ”余杭 辉 说 : “ 真的 很 感谢 他 啊 。 ”今天 下午 , 浙江 新闻 客户端 记者 联系 上 了 蒋 师傅 , 他 说 那天 自己 正好 休息 在家 。“ 我 听到 有人 喊救命 , 就 跳下去 帮忙 了 。 ”蒋 师傅 今年 47 岁 , 从 小 就 在 这个 河边 长大 。蒋 师傅 说 , 那 天水 真的 很 大 , 他 的 个子 有 1 米 78 , 但 却 站 不 到底 。把 人 就 救上来 后 , 蒋 师傅 的 妻子 和 邻居 们 将 自己 的 干净 衣服 给 余杭 辉 一家人 换上 。因为 当天 早晨 天气 有点冷 , 他 还 开车 将 余杭 辉 的 老妈 和 孩子 送 回 了 家 。“ 虽然 事情 已经 过 了 好 几天 , 但 他 一直 没 跟 单位 说 过 , 要不是 今天 有人 来 送 锦旗 , 我们 真的 不 了解 。 ”金华 市 公交 集团 运营 二 公司 经理 吴卫 洪 说 。\n",
      "\n",
      "模型生成的摘要：\n",
      "2020年6月29日，余杭辉带着锦旗到金华公交二公司感谢，蒋永泉师傅救了余辉一家人的命。\n",
      "\n",
      "参考摘要：\n",
      "[ 金东区 ] 暴雨 致 道路 积水 淹没 小汽车 , 一家 四口 被困 车内 , 公交车 司机 救下 一家人 ( 图 )\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "初步测试的摘要评估结果：\n",
      "ROUGE-1 Recall Score: 0.4458\n",
      "ROUGE-2 Recall Score: 0.2097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 进行初步测试，处理前10篇文章并评估\n",
    "def initial_test_and_evaluation(articles, references, batch_size=2, test_limit=10):\n",
    "    print(\"正在进行初步测试...\\n\")\n",
    "    generated_summaries_test = generate_summaries_in_batches(\n",
    "        articles, \n",
    "        references,\n",
    "        batch_size=batch_size,    \n",
    "        test=True, \n",
    "        test_limit=test_limit    \n",
    "    )\n",
    "    \n",
    "    # 计算初步测试的ROUGE分数\n",
    "    rouge = Rouge()\n",
    "    # 对摘要和参考摘要进行预处理：按字符切分，并用空格连接\n",
    "    preprocessed_generated = [' '.join(list(summary)) for summary in generated_summaries_test]\n",
    "    preprocessed_references = [' '.join(list(ref)) for ref in references[:test_limit]]\n",
    "    \n",
    "    scores = rouge.get_scores(preprocessed_generated, preprocessed_references, avg=True)\n",
    "    \n",
    "    print(f\"初步测试的摘要评估结果：\")\n",
    "    print(f\"ROUGE-1 Recall Score: {scores['rouge-1']['r']:.4f}\")\n",
    "    print(f\"ROUGE-2 Recall Score: {scores['rouge-2']['r']:.4f}\")\n",
    "    \n",
    "# 运行初步测试\n",
    "initial_test_and_evaluation(\n",
    "    sum_articles[:10], \n",
    "    sum_references[:10],\n",
    "    batch_size=2,    \n",
    "    test_limit=10    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案例分析\n",
    "\n",
    "在初步测试中，我对10篇文章进行了摘要生成，整体效果较好。模型能够准确地识别文章中的主要人物、事件、时间和地点等关键信息，生成的摘要句子通顺，并且基本能够概括原文的核心内容。然而，仍存在一些问题和改进空间：\n",
    "\n",
    "## 问题分析\n",
    "\n",
    "1. **情感理解不足**：\n",
    "   在某些文章中，LLM未能完全理解原文中蕴含的“情感”。例如，在文章2中，原文描述了老人之间深厚的感情，但LLM生成的摘要未能传达这一情感元素。这表明，虽然LLM能够识别并提取事实信息，但对情感或隐含信息的理解较为有限。\n",
    "\n",
    "2. **信息选择性不足**：\n",
    "   在文章6中，原文提到贪污官员自称月薪8千，虽然这一信息看似细节不重要，但在参考摘要中作为衬托加入，增添了文章的深度。相比之下，LLM未能将这一信息纳入摘要。这说明，LLM在处理信息的选择性时，可能过于简化，忽视了一些有助于理解文章语境的细节。\n",
    "\n",
    "3. **摘要简洁性不足**：\n",
    "   尽管LLM生成的摘要已经在简洁性方面进行了优化，基本概括了全文内容，但与人类编写的新闻摘要相比，仍显得不够简练。LLM生成的摘要通常包含过多的背景信息和次要细节，尽管这些内容在某些情况下有助于理解，但对于摘要而言，它们可能显得冗长。这可能是由于模型的参数量限制，LLM未能在生成时做到更激进的精简，无法去除多余的信息，导致摘要显得相对冗长。\n",
    "\n",
    "## 提升空间\n",
    "\n",
    "1. **增强情感理解**：\n",
    "   为了让LLM能够更好地理解文章中的情感或隐含的情绪，可以通过增加情感识别模块或利用情感标注的训练数据对模型进行微调。这将有助于提升摘要生成时对情感层面的捕捉能力，特别是在涉及人物关系或情感色彩较强的文章中。\n",
    "\n",
    "2. **优化信息选择性**：\n",
    "   模型可以通过强化信息选择的策略，进一步优化摘要的生成。例如，可以设计针对特定细节（如人物对话、背景细节等）的权重调整机制，以便在生成摘要时自动考虑是否应当保留某些看似次要但富有内涵的信息。\n",
    "\n",
    "3. **提高摘要简洁性**：\n",
    "   进一步增强摘要的简洁性是未来的优化方向。通过对LLM模型的参数进行调整，或者使用更精简的Prompt设计，可以引导模型在生成摘要时更加注重精炼、突出重点，并减少冗余内容。此外，可以探索更高效的抽取式摘要方法，以提高摘要的紧凑度和信息密度。\n",
    "\n",
    "4. **增强模型参数与上下文处理能力**：\n",
    "   随着模型规模的扩大，LLM在处理复杂任务时的能力将得到提升。未来，可以考虑使用更大规模的模型或进行更多的领域微调，以提高对长文本的理解和总结能力。\n",
    "\n",
    "通过以上改进，应该能够进一步提升LLM生成摘要的质量，使其更加精确、简洁且具备更强的上下文理解能力。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始进行完整评估...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:   0%|                                                                   | 0/1000 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Generating Summaries: 100%|██████████████████████████████████████████████████████| 1000/1000 [7:18:05<00:00, 26.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "摘要任务评估结果：\n",
      "ROUGE-1 Recall Score: 0.5603\n",
      "ROUGE-2 Recall Score: 0.3029\n"
     ]
    }
   ],
   "source": [
    "# 进行完整评估\n",
    "def full_evaluation_all(sum_articles, sum_references, batch_size=2):\n",
    "    # 使用所有文章进行评估\n",
    "    sample_articles = sum_articles\n",
    "    sample_references = sum_references\n",
    "    \n",
    "    # 使用所有文章生成摘要\n",
    "    print(\"\\n开始进行完整评估...\\n\")\n",
    "    generated_summaries = generate_summaries_in_batches(\n",
    "        sample_articles, \n",
    "        sample_references, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # 计算 ROUGE 分数\n",
    "    rouge = Rouge()\n",
    "    # 对摘要和参考摘要进行预处理：按字符切分，并用空格连接\n",
    "    preprocessed_generated = [' '.join(list(summary)) for summary in generated_summaries]\n",
    "    preprocessed_references = [' '.join(list(ref)) for ref in sample_references]\n",
    "    \n",
    "    scores = rouge.get_scores(preprocessed_generated, preprocessed_references, avg=True)\n",
    "    \n",
    "    print(f\"摘要任务评估结果：\")\n",
    "    print(f\"ROUGE-1 Recall Score: {scores['rouge-1']['r']:.4f}\")\n",
    "    print(f\"ROUGE-2 Recall Score: {scores['rouge-2']['r']:.4f}\")\n",
    "\n",
    "full_evaluation_all(sum_articles, sum_references, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3基于LLM的摘要结果分析\n",
    "\n",
    "## 1. LLM模型生成结果\n",
    "在使用LLM对summary.jsonl中的所有文章进行摘要生成后，我得到了以下评估结果：\n",
    "\n",
    "- **ROUGE-1 Recall Score**: 0.5603\n",
    "- **ROUGE-2 Recall Score**: 0.3029\n",
    "\n",
    "从ROUGE得分来看，LLM的摘要在召回率上表现较好，超过了**满分标准**。特别是在ROUGE-1上，取得了超过0.5的得分，意味着LLM能够很好地覆盖原文中的重要词汇和短语，尤其是在提取式摘要中，它能有效地选择关键字进行概括。ROUGE-2得分则相对较低，说明LLM在处理较复杂的二元组（bigram）匹配时相对较弱，可能是在较长句子结构的生成上存在一些信息丢失，但也超过了满分标准，说明这个基于LLM的摘要任务实现的较好。\n",
    "\n",
    "## 2. 基于神经网络模型与基于LLM的摘要效果对比分析\n",
    "### 优点与局限性\n",
    "- **LLM模型优点**：\n",
    "  1. **语言理解能力强**：LLM能够更好地理解上下文，并生成流畅自然的文本，能够概括较为复杂的内容。\n",
    "  2. **鲁棒性强**：LLM在面对不同类型的文本时具有较强的适应能力，不仅可以提取关键事件，还能处理较为复杂的长文本摘要。\n",
    "  \n",
    "- **神经网络模型优点**：\n",
    "  1. **更高的计算效率**：传统的神经网络模型通常计算效率较高，能够快速生成摘要。\n",
    "  2. **结构化摘要**：神经网络模型在处理结构化数据时（如新闻报道或技术文档）可能生成更加简洁和结构化的摘要。\n",
    "  \n",
    "- **LLM模型的局限性**：\n",
    "  1. **生成摘要的简洁性不足**：虽然LLM在语言生成方面表现较好，但由于其生成的摘要往往包含较多的冗余信息，导致简洁度不足。这是LLM模型需要进一步优化的方向。\n",
    "  2. **情感和细节捕捉不足**：LLM有时不能有效传递文章中的情感或细节，尤其是对于复杂的情感信息（例如人物情感、讽刺或隐含的意图）较难处理。\n",
    "\n",
    "- **神经网络模型的局限性**：\n",
    "  1. **准确性不足**：神经网络模型在处理较复杂的上下文关系时，可能会漏掉一些重要的关键信息，影响摘要的准确性。\n",
    "  2. **语法和流畅性**：传统神经网络模型生成的摘要可能存在语法不流畅、句子生硬的问题，尤其是在面对长文本时。\n",
    "\n",
    "## 3. 总结\n",
    "通过对基于LLM的摘要结果进行分析：\n",
    "\n",
    "- LLM模型在摘要生成任务中表现出较强的语言理解和生成能力，尤其是在召回率（ROUGE-1）上表现优异。最终超过了**满分标准**：\"ROUGE-1 为0.4以上，ROUGE-2 为0.2以上\"\n",
    "- 尽管LLM模型在信息覆盖上优于传统神经网络模型，但在简洁性和细节把握上仍有待改进。\n",
    "- 为了进一步提升摘要的质量，可以结合LLM的语言生成能力与神经网络模型的效率，优化摘要生成的过程，尤其是针对冗长、情感和细节捕捉等方面进行改进。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
